{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fab730f",
   "metadata": {},
   "source": [
    "# Training Transformer Models for Text Classification\n",
    "\n",
    "## Introduction\n",
    "This notebook provides a hands-on guide to using Hugging Face transformer models for text classification tasks.\n",
    "\n",
    "You will learn how to leverage pre-trained transformer models like DistilBERT to build an emotion classification system that can identify emotions from Twitter messages.\n",
    "\n",
    "Throughout this tutorial, we'll explore two main approaches to text classification with transformers: feature extraction (using transformer embeddings with a simple classifier) and fine-tuning (training the entire model end-to-end). You'll gain practical experience working with the Hugging Face ecosystem, including the `transformers` and `datasets` libraries.\n",
    "\n",
    "‚ö†Ô∏è If your computer is slow, you can run this notebook on Google Colab by downloading it and running it there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ff6f99",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "- Load and explore datasets from the Hugging Face Hub for NLP tasks\n",
    "- Understand tokenization strategies including character, word, and subword tokenization\n",
    "- Use pre-trained transformers as feature extractors to generate embeddings for downstream tasks\n",
    "- Fine-tune transformer models for custom classification problems using the Trainer API\n",
    "- Evaluate model performance using appropriate metrics and confusion matrices\n",
    "- Perform error analysis to identify model weaknesses and dataset issues\n",
    "- Visualize high-dimensional embeddings using dimensionality reduction techniques (UMAP)\n",
    "- Save and share models on the Hugging Face Hub for deployment\n",
    "- Compare trade-offs between feature-based and fine-tuning approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d896ca51",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "- Basic knowledge of Python and machine learning concepts\n",
    "- Familiarity with PyTorch or TensorFlow (helpful but not required)\n",
    "- Understanding of classification tasks and evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4768db0",
   "metadata": {},
   "source": [
    "## The Dataset\n",
    "To build our emotion detector we'll use a great dataset from an article that explored how emotions are represented in English Twitter messages.footnote:[E. Saravia et al., \"CARER: Contextualized Affect Representations for Emotion Recognition,\" Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (Oct‚ÄìNov 2018): 3687‚Äì3697, http://dx.doi.org/10.18653/v1/D18-1404.] Unlike most sentiment analysis datasets that involve just \"positive\" and \"negative\" polarities, this dataset contains six basic emotions: anger, disgust, fear, joy, sadness, and surprise. Given a tweet, our task will be to train a model that can classify it into one of these emotions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2144a1e0",
   "metadata": {},
   "source": [
    "### A First Look at Hugging Face Datasets\n",
    "We will use `datasets` to download the data from the Hugging Face Hub. We can use the `list_datasets()` function to see what datasets are available on the Hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e12da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import list_datasets\n",
    "\n",
    "all_datasets = list_datasets()\n",
    "print(f\"There are {len(all_datasets)} datasets currently available on the Hub\")\n",
    "print(f\"The first 10 are: {all_datasets[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8947750c",
   "metadata": {},
   "source": [
    "We see that each dataset is given a name, so let's load the emotion dataset with the `load_dataset()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f7d546",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "emotions = load_dataset(\"emotion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b53960e",
   "metadata": {},
   "source": [
    "If we look inside our `emotions` object we can inspect the available splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ca00fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6733b8",
   "metadata": {},
   "source": [
    "The dataset behaves similarly to a Python dictionary, with each key corresponding to a different split. We can use the usual dictionary syntax to access an individual split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b120d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = emotions[\"train\"]\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7eb27d",
   "metadata": {},
   "source": [
    "The `Dataset` object behaves like an array, so we can query its length or access rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d8fae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeee2f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ce5347",
   "metadata": {},
   "source": [
    "Column names and features can also be inspected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a46d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0a4bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_ds.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b495c7",
   "metadata": {},
   "source": [
    "We can retrieve multiple rows or entire columns as lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a032a3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_ds[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b68dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_ds[\"text\"][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbebf789",
   "metadata": {},
   "source": [
    "üéØ **Exercise 1: Dataset Exploration**\n",
    "\n",
    "Now that you've seen how to load and explore the emotion dataset, try the following:\n",
    "\n",
    "1. Browse the Hugging Face Datasets Hub and find another text classification dataset (e.g., `imdb`, `ag_news`, or `yelp_review_full`). Load this dataset and explore its structure. How many classes does it have? How is it different from the emotion dataset?\n",
    "2. The current dataset is imbalanced. Using the Pandas documentation, research and implement at least one strategy to handle class imbalance (e.g., using `resample()` or `sample()` with weights). What effect do you expect this to have on model performance?\n",
    "\n",
    "üí° Hint: Check the Hugging Face Datasets documentation for loading different datasets and the imbalanced-learn documentation for sampling strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ab9445",
   "metadata": {},
   "source": [
    "### Sidebar: What If My Dataset Is Not on the Hub?\n",
    "We'll be using the Hugging Face Hub to download datasets for most of the examples in this book. But in many cases, you'll find yourself working with data that is either stored on your laptop or on a remote server in your organization. Datasets provides several loading scripts to handle local and remote datasets. Examples for the most common data formats are shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04529478",
   "metadata": {},
   "source": [
    "| Data format | Loading script | Example |\n",
    "| --- | --- | --- |\n",
    "| CSV | `csv` | `load_dataset(\"csv\", data_files=\"my_file.csv\")` |\n",
    "| Text | `text` | `load_dataset(\"text\", data_files=\"my_file.txt\")` |\n",
    "| JSON | `json` | `load_dataset(\"json\", data_files=\"my_file.jsonl\")` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417380d9",
   "metadata": {},
   "source": [
    "As you can see for each data format, we just need to pass the relevant loading script to the `load_dataset()` function, along with a `data_files` argument that specifies the path or URL to one or more files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c599c395",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_url = \"https://huggingface.co/datasets/transformersbook/emotion-train-split/raw/main/train.txt\"\n",
    "!wget {dataset_url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f5445e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 1 train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48b943f",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_local = load_dataset(\"csv\", data_files=\"train.txt\", sep=\";\", \n",
    "                              names=[\"text\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3d4adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_url = \"https://huggingface.co/datasets/transformersbook/emotion-train-split/raw/main/train.txt\"\n",
    "emotions_remote = load_dataset(\"csv\", data_files=dataset_url, sep=\";\", \n",
    "                               names=[\"text\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c20f52f",
   "metadata": {},
   "source": [
    "### From Datasets to DataFrames\n",
    "Although `datasets` provides a lot of low-level functionality to slice and dice our data, it is often convenient to convert a `Dataset` object to a Pandas `DataFrame` so we can access high-level APIs for data visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c5239f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "emotions.set_format(type=\"pandas\")\n",
    "df = emotions[\"train\"][:]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb48200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_int2str(row):\n",
    "    return emotions[\"train\"].features[\"label\"].int2str(row)\n",
    "\n",
    "df[\"label_name\"] = df[\"label\"].apply(label_int2str)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e415daa8",
   "metadata": {},
   "source": [
    "### Looking at the Class Distribution\n",
    "Whenever you are working on text classification problems, it is a good idea to examine the distribution of examples across the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f15aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(df[\"label_name\"].value_counts(ascending=True)\n",
    "    .plot.barh())\n",
    "plt.title(\"Frequency of Classes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b92b014",
   "metadata": {},
   "source": [
    "In this case, we can see that the dataset is heavily imbalanced; the `joy` and `sadness` classes appear frequently, whereas `love` and `surprise` are about 5‚Äì10 times rarer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c81f04",
   "metadata": {},
   "source": [
    "### How Long Are Our Tweets?\n",
    "Transformer models have a maximum input sequence length that is referred to as the maximum context size. For applications using DistilBERT, the maximum context size is 512 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9817fb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Words Per Tweet\"] = df[\"text\"].str.split().apply(len)\n",
    "df.boxplot(\"Words Per Tweet\", by=\"label_name\", grid=False, showfliers=False,\n",
    "           color=\"black\")\n",
    "plt.suptitle(\"\")\n",
    "plt.xlabel(\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4bceb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions.reset_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7867878",
   "metadata": {},
   "source": [
    "## From Text to Tokens\n",
    "Transformer models like DistilBERT cannot receive raw strings as input; instead, they assume the text has been tokenized and encoded as numerical vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b15ca83",
   "metadata": {},
   "source": [
    "### Character Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da905d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Tokenizing text is a core task of NLP.\"\n",
    "tokenized_text = list(text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce6996e",
   "metadata": {},
   "outputs": [],
   "source": [
    "token2idx = {ch: idx for idx, ch in enumerate(sorted(set(tokenized_text)))}\n",
    "print(token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a1679e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = [token2idx[token] for token in tokenized_text]\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a13af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "input_ids = torch.tensor(input_ids)\n",
    "one_hot_encodings = F.one_hot(input_ids, num_classes=len(token2idx))\n",
    "one_hot_encodings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6006f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Token: {tokenized_text[0]}\")\n",
    "print(f\"Tensor index: {input_ids[0]}\")\n",
    "print(f\"One-hot: {one_hot_encodings[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb804765",
   "metadata": {},
   "source": [
    "### Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf75f210",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = text.split()\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d0f0c2",
   "metadata": {},
   "source": [
    "### Subword Tokenization\n",
    "We'll use the tokenizer associated with DistilBERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457c102d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b33a40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = tokenizer(text)\n",
    "print(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4238b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(encoded_text.input_ids)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab9992c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.convert_tokens_to_string(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2f4952",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.vocab_size)\n",
    "print(tokenizer.model_max_length)\n",
    "print(tokenizer.model_input_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc0b142",
   "metadata": {},
   "source": [
    "## Tokenizing the Whole Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a2a1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True)\n",
    "\n",
    "print(tokenize(emotions[\"train\"][:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab172d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens2ids = list(zip(tokenizer.all_special_tokens, tokenizer.all_special_ids))\n",
    "data = sorted(tokens2ids, key=lambda x: x[-1])\n",
    "df_special = pd.DataFrame(data, columns=[\"Special Token\", \"Special Token ID\"])\n",
    "df_special.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f23c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_encoded = emotions.map(tokenize, batched=True, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a1f777",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(emotions_encoded[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d81524f",
   "metadata": {},
   "source": [
    "## Training a Text Classifier\n",
    "Models like DistilBERT are pretrained to predict masked words in text. To use them for text classification we need to modify them slightly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6efcd0",
   "metadata": {},
   "source": [
    "### Transformers as Feature Extractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc23d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModel.from_pretrained(model_ckpt).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379edce9",
   "metadata": {},
   "source": [
    "üéØ **Exercise 2: Tokenization Experiments**\n",
    "\n",
    "1. Load a different pre-trained model tokenizer (e.g., `bert-base-cased`, `roberta-base`, or `albert-base-v2`) and compare its tokenization output with DistilBERT. What differences do you notice? How does cased vs uncased tokenization affect the output?\n",
    "2. Experiment with the tokenizer's padding and truncation parameters. What happens if you set `max_length=10` with `truncation=True`? Read the tokenizer documentation to understand different padding strategies (`max_length`, `longest`, `do_not_pad`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcbc818",
   "metadata": {},
   "source": [
    "### Extracting the Last Hidden States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1f1b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"this is a test\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "print(f\"Input tensor shape: {inputs['input_ids'].size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfccc9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862e2f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.last_hidden_state.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b627d6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.last_hidden_state[:, 0].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acaf50d",
   "metadata": {},
   "source": [
    "Now let's extract the hidden states for the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9d35ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hidden_states(batch):\n",
    "    inputs = {k: v.to(device) for k, v in batch.items() if k in tokenizer.model_input_names}\n",
    "    with torch.no_grad():\n",
    "        last_hidden_state = model(**inputs).last_hidden_state\n",
    "    return {\"hidden_state\": last_hidden_state[:, 0].cpu().numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2ee6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_encoded.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21210e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_hidden = emotions_encoded.map(extract_hidden_states, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac67d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_hidden[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d037c2e8",
   "metadata": {},
   "source": [
    "### Creating a Feature Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b682731e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_train = np.array(emotions_hidden[\"train\"][\"hidden_state\"])\n",
    "X_valid = np.array(emotions_hidden[\"validation\"][\"hidden_state\"])\n",
    "y_train = np.array(emotions_hidden[\"train\"][\"label\"])\n",
    "y_valid = np.array(emotions_hidden[\"validation\"][\"label\"])\n",
    "X_train.shape, X_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dac8c6",
   "metadata": {},
   "source": [
    "### Visualizing the Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090b0d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X_scaled = MinMaxScaler().fit_transform(X_train)\n",
    "mapper = UMAP(n_components=2, metric=\"cosine\").fit(X_scaled)\n",
    "df_emb = pd.DataFrame(mapper.embedding_, columns=[\"X\", \"Y\"])\n",
    "df_emb[\"label\"] = y_train\n",
    "df_emb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a799fc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(7, 5))\n",
    "axes = axes.flatten()\n",
    "cmaps = [\"Greys\", \"Blues\", \"Oranges\", \"Reds\", \"Purples\", \"Greens\"]\n",
    "labels = emotions[\"train\"].features[\"label\"].names\n",
    "\n",
    "for i, (label, cmap) in enumerate(zip(labels, cmaps)):\n",
    "    df_emb_sub = df_emb.query(f\"label == {i}\")\n",
    "    axes[i].hexbin(df_emb_sub[\"X\"], df_emb_sub[\"Y\"], cmap=cmap,\n",
    "                   gridsize=20, linewidths=(0,))\n",
    "    axes[i].set_title(label)\n",
    "    axes[i].set_xticks([])\n",
    "    axes[i].set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9747b9",
   "metadata": {},
   "source": [
    "### Training a Simple Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e205d0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_clf = LogisticRegression(max_iter=3000)\n",
    "lr_clf.fit(X_train, y_train)\n",
    "lr_clf.score(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3fb015",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "dummy_clf.score(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b44ce87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(y_preds, y_true, labels):\n",
    "    cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n",
    "    plt.title(\"Normalized confusion matrix\")\n",
    "    plt.show()\n",
    "    \n",
    "y_preds = lr_clf.predict(X_valid)\n",
    "plot_confusion_matrix(y_preds, y_valid, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4df1b4",
   "metadata": {},
   "source": [
    "## Fine-Tuning Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea9ab87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "num_labels = 6\n",
    "model = (AutoModelForSequenceClassification\n",
    "         .from_pretrained(model_ckpt, num_labels=num_labels)\n",
    "         .to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29859879",
   "metadata": {},
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af179fdf",
   "metadata": {},
   "source": [
    "üéØ **Exercise 3: Feature Extraction and Model Selection**\n",
    "\n",
    "1. Try using different classifiers from scikit-learn instead of logistic regression. Test at least two of the following: `RandomForestClassifier`, `SVC`, or `MLPClassifier`. Compare their performance with logistic regression. Which one works best and why?\n",
    "2. Experiment with the UMAP configuration (`n_components`, `metric`, `n_neighbors`). How do these changes affect the visualization and what insights can you gain?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422dbe6f",
   "metadata": {},
   "source": [
    "Log in to your account on the Hugging Face Hub to push the fine-tuned model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c63f033",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# notebook_login()  # Uncomment to log in from a notebook environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df38246",
   "metadata": {},
   "source": [
    "Define the training arguments and initialize the `Trainer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e8ba68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "batch_size = 64\n",
    "logging_steps = len(emotions_encoded[\"train\"]) // batch_size\n",
    "model_name = f\"{model_ckpt}-finetuned-emotion\"\n",
    "training_args = TrainingArguments(output_dir=model_name,\n",
    "                                  num_train_epochs=2,\n",
    "                                  learning_rate=2e-5,\n",
    "                                  per_device_train_batch_size=batch_size,\n",
    "                                  per_device_eval_batch_size=batch_size,\n",
    "                                  weight_decay=0.01,\n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  disable_tqdm=False,\n",
    "                                  logging_steps=logging_steps,\n",
    "                                  push_to_hub=True,\n",
    "                                  log_level=\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70eea90",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model, args=training_args, \n",
    "                  compute_metrics=compute_metrics,\n",
    "                  train_dataset=emotions_encoded[\"train\"],\n",
    "                  eval_dataset=emotions_encoded[\"validation\"],\n",
    "                  tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a063b339",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e75e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_output = trainer.predict(emotions_encoded[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44458bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_output.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510fa8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = np.argmax(preds_output.predictions, axis=1)\n",
    "plot_confusion_matrix(y_preds, y_valid, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108f47fb",
   "metadata": {},
   "source": [
    "üéØ **Exercise 4: Fine-Tuning Hyperparameters**\n",
    "\n",
    "1. Modify the `TrainingArguments` to experiment with different hyperparameters (e.g., learning rate, epochs, batch size, weight decay, warmup steps). Which combination gives the best F1-score?\n",
    "2. Try fine-tuning a different pre-trained model (e.g., `bert-base-uncased`, `roberta-base`, or `albert-base-v2`). How does the training time and final performance compare?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064022f9",
   "metadata": {},
   "source": [
    "### Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734a732b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "def forward_pass_with_label(batch):\n",
    "    inputs = {k: v.to(device) for k, v in batch.items() if k in tokenizer.model_input_names}\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs)\n",
    "        pred_label = torch.argmax(output.logits, axis=-1)\n",
    "        loss = cross_entropy(output.logits, batch[\"label\"].to(device), reduction=\"none\")\n",
    "    return {\"loss\": loss.cpu().numpy(), \n",
    "            \"predicted_label\": pred_label.cpu().numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb073dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_encoded.set_format(\"torch\", \n",
    "                            columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "emotions_encoded[\"validation\"] = emotions_encoded[\"validation\"].map(\n",
    "    forward_pass_with_label, batched=True, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa00780",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_encoded.set_format(\"pandas\")\n",
    "cols = [\"text\", \"label\", \"predicted_label\", \"loss\"]\n",
    "df_test = emotions_encoded[\"validation\"][:][cols]\n",
    "df_test[\"label\"] = df_test[\"label\"].apply(label_int2str)\n",
    "df_test[\"predicted_label\"] = df_test[\"predicted_label\"].apply(label_int2str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49ee874",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.sort_values(\"loss\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905ea36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.sort_values(\"loss\", ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bb437e",
   "metadata": {},
   "source": [
    "üéØ **Exercise 6: Advanced Evaluation Metrics**\n",
    "\n",
    "1. Implement additional evaluation metrics (e.g., per-class precision/recall, macro vs micro vs weighted F1-score, Cohen's Kappa, MCC). Which metric would you prioritize for this task and why?\n",
    "2. Create a function that extracts the top 10 most misclassified examples (where the model was most confident but wrong), identifies patterns, and suggests data augmentation strategies to improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0411e26e",
   "metadata": {},
   "source": [
    "## Saving and Sharing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d795279c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub(commit_message=\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738ac967",
   "metadata": {},
   "source": [
    "Use the fine-tuned model with the `pipeline()` API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d0deff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_id = \"transformersbook/distilbert-base-uncased-finetuned-emotion\"\n",
    "classifier = pipeline(\"text-classification\", model=model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21de7956",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_tweet = \"I saw a movie today and it was really good.\"\n",
    "preds = classifier(custom_tweet, return_all_scores=True)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e92574",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df = pd.DataFrame(preds[0])\n",
    "plt.bar(labels, 100 * preds_df[\"score\"], color='C0')\n",
    "plt.title(f'\"{custom_tweet}\"')\n",
    "plt.ylabel(\"Class probability (%)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0609b65",
   "metadata": {},
   "source": [
    "üéØ **Exercise 5: Model Deployment and Inference**\n",
    "\n",
    "1. Read about the Hugging Face Inference API and test your deployed model via HTTP requests. Write a Python function using the `requests` library to send text to your model and receive predictions. How would you integrate this into a web application?\n",
    "2. Explore different parameters of the `pipeline()` API such as `top_k`, `truncation`, and `max_length`. Try creating a pipeline for another task (e.g., `sentiment-analysis` or `zero-shot-classification`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59caa571",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Congratulations, you now know how to train a transformer model to classify the emotions in tweets! We have seen two complementary approaches based on features and fine-tuning, and investigated their strengths and weaknesses. Continue exploring by deploying models, speeding them up, and expanding to multilingual or low-resource settings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
