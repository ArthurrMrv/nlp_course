{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Advanced Natural Language Processing\n",
        "\n",
        "This notebook covers advanced NLP techniques including tokenization, stemming, lemmatization, named entity recognition, text preprocessing, and vectorization methods.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Release of Punkt Tokenizer ðŸ“¦\n",
        "\n",
        "Before we begin, let's ensure all required NLTK data is downloaded.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ punkt is already downloaded\n",
            "âœ“ punkt_tab is already downloaded\n",
            "âœ“ stopwords is already downloaded\n",
            "âœ“ averaged_perceptron_tagger is already downloaded\n",
            "âœ“ averaged_perceptron_tagger_eng is already downloaded\n",
            "Downloading wordnet...\n",
            "âœ“ wordnet downloaded successfully\n",
            "âœ“ maxent_ne_chunker is already downloaded\n",
            "Downloading maxent_ne_chunker_tab...\n",
            "âœ“ maxent_ne_chunker_tab downloaded successfully\n",
            "âœ“ words is already downloaded\n",
            "\n",
            "All required NLTK data is ready!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk_data_packages = ['punkt', 'punkt_tab', 'stopwords', 'averaged_perceptron_tagger', 'averaged_perceptron_tagger_eng', 'wordnet', 'maxent_ne_chunker', 'maxent_ne_chunker_tab', 'words']\n",
        "\n",
        "for package in nltk_data_packages:\n",
        "    try:\n",
        "        if package == 'punkt':\n",
        "            nltk.data.find('tokenizers/punkt')\n",
        "        elif package == 'punkt_tab':\n",
        "            nltk.data.find('tokenizers/punkt_tab')\n",
        "        elif package == 'stopwords':\n",
        "            nltk.data.find('corpora/stopwords')\n",
        "        elif package == 'averaged_perceptron_tagger':\n",
        "            nltk.data.find('taggers/averaged_perceptron_tagger')\n",
        "        elif package == 'averaged_perceptron_tagger_eng':\n",
        "            nltk.data.find('taggers/averaged_perceptron_tagger_eng')\n",
        "        elif package == 'wordnet':\n",
        "            nltk.data.find('corpora/wordnet')\n",
        "        elif package == 'maxent_ne_chunker':\n",
        "            nltk.data.find('chunkers/maxent_ne_chunker')\n",
        "        elif package == 'maxent_ne_chunker_tab':\n",
        "            nltk.data.find('chunkers/maxent_ne_chunker_tab')\n",
        "        elif package == 'words':\n",
        "            nltk.data.find('corpora/words')\n",
        "        print(f\"âœ“ {package} is already downloaded\")\n",
        "    except LookupError:\n",
        "        print(f\"Downloading {package}...\")\n",
        "        nltk.download(package, quiet=True)\n",
        "        print(f\"âœ“ {package} downloaded successfully\")\n",
        "\n",
        "print(\"\\nAll required NLTK data is ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tokenization\n",
        "\n",
        "**Tokenization** is the process of breaking down text into smaller units called tokens. These tokens can be words, sentences, or subwords.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sentence Tokenization\n",
        "\n",
        "Splitting text into individual sentences.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original text:\n",
            "Natural Language Processing is fascinating. It enables machines to understand human language. NLP has many applications in today's world.\n",
            "\n",
            "Sentences:\n",
            "1. Natural Language Processing is fascinating.\n",
            "2. It enables machines to understand human language.\n",
            "3. NLP has many applications in today's world.\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "# Example text with multiple sentences\n",
        "text = \"Natural Language Processing is fascinating. It enables machines to understand human language. NLP has many applications in today's world.\"\n",
        "\n",
        "# Sentence tokenization\n",
        "sentences = sent_tokenize(text)\n",
        "print(\"Original text:\")\n",
        "print(text)\n",
        "print(\"\\nSentences:\")\n",
        "for i, sentence in enumerate(sentences, 1):\n",
        "    print(f\"{i}. {sentence}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Word Tokenization\n",
        "\n",
        "Splitting text into individual words.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original text: Don't judge a book by its cover\n",
            "Tokens: ['Do', \"n't\", 'judge', 'a', 'book', 'by', 'its', 'cover']\n",
            "\n",
            "Number of tokens: 8\n",
            "\n",
            "==================================================\n",
            "Multi-sentence tokenization:\n",
            "Sentence: Hello world!\n",
            "Tokens: ['Hello', 'world', '!']\n",
            "\n",
            "Sentence: How are you?\n",
            "Tokens: ['How', 'are', 'you', '?']\n",
            "\n",
            "Sentence: I'm doing great.\n",
            "Tokens: ['I', \"'m\", 'doing', 'great', '.']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Word tokenization\n",
        "text = \"Don't judge a book by its cover\"\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "print(\"Original text:\", text)\n",
        "print(\"Tokens:\", tokens)\n",
        "print(f\"\\nNumber of tokens: {len(tokens)}\")\n",
        "\n",
        "# Tokenize each sentence\n",
        "multi_sentence = \"Hello world! How are you? I'm doing great.\"\n",
        "sentences = sent_tokenize(multi_sentence)\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Multi-sentence tokenization:\")\n",
        "for sentence in sentences:\n",
        "    tokens = word_tokenize(sentence)\n",
        "    print(f\"Sentence: {sentence}\")\n",
        "    print(f\"Tokens: {tokens}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Stemming\n",
        "\n",
        "**Stemming** is the process of reducing words to their root form by removing suffixes. It's a crude heuristic process that chops off the ends of words.\n",
        "\n",
        "## Types of Stemmers\n",
        "\n",
        "1. **Porter Stemmer**: Most common, aggressive stemming\n",
        "2. **Snowball Stemmer**: Improved version of Porter, supports multiple languages\n",
        "3. **Lancaster Stemmer**: More aggressive than Porter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Porter Stemmer:\n",
            "--------------------------------------------------\n",
            "running         -> run\n",
            "runs            -> run\n",
            "runner          -> runner\n",
            "ran             -> ran\n",
            "beautifully     -> beauti\n",
            "beautiful       -> beauti\n",
            "happily         -> happili\n",
            "happiness       -> happi\n",
            "unhappiness     -> unhappi\n",
            "\n",
            "Snowball Stemmer:\n",
            "--------------------------------------------------\n",
            "running         -> run\n",
            "runs            -> run\n",
            "runner          -> runner\n",
            "ran             -> ran\n",
            "beautifully     -> beauti\n",
            "beautiful       -> beauti\n",
            "happily         -> happili\n",
            "happiness       -> happi\n",
            "unhappiness     -> unhappi\n",
            "\n",
            "Lancaster Stemmer:\n",
            "--------------------------------------------------\n",
            "running         -> run\n",
            "runs            -> run\n",
            "runner          -> run\n",
            "ran             -> ran\n",
            "beautifully     -> beauty\n",
            "beautiful       -> beauty\n",
            "happily         -> happy\n",
            "happiness       -> happy\n",
            "unhappiness     -> unhappy\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer\n",
        "\n",
        "# Example words\n",
        "words = [\"running\", \"runs\", \"runner\", \"ran\", \"beautifully\", \"beautiful\", \"happily\", \"happiness\", \"unhappiness\"]\n",
        "\n",
        "# Porter Stemmer\n",
        "porter = PorterStemmer()\n",
        "print(\"Porter Stemmer:\")\n",
        "print(\"-\" * 50)\n",
        "for word in words:\n",
        "    stemmed = porter.stem(word)\n",
        "    print(f\"{word:15} -> {stemmed}\")\n",
        "\n",
        "# Snowball Stemmer (English)\n",
        "snowball = SnowballStemmer('english')\n",
        "print(\"\\nSnowball Stemmer:\")\n",
        "print(\"-\" * 50)\n",
        "for word in words:\n",
        "    stemmed = snowball.stem(word)\n",
        "    print(f\"{word:15} -> {stemmed}\")\n",
        "\n",
        "# Lancaster Stemmer (more aggressive)\n",
        "lancaster = LancasterStemmer()\n",
        "print(\"\\nLancaster Stemmer:\")\n",
        "print(\"-\" * 50)\n",
        "for word in words:\n",
        "    stemmed = lancaster.stem(word)\n",
        "    print(f\"{word:15} -> {stemmed}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stemming in Practice\n",
        "\n",
        "Applying stemming to a sentence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original sentence: The runners were running quickly and beautifully through the beautiful garden\n",
            "\n",
            "Original tokens: ['the', 'runners', 'were', 'running', 'quickly', 'and', 'beautifully', 'through', 'the', 'beautiful', 'garden']\n",
            "Stemmed tokens: ['the', 'runner', 'were', 'run', 'quickli', 'and', 'beauti', 'through', 'the', 'beauti', 'garden']\n",
            "\n",
            "Stemmed sentence: the runner were run quickli and beauti through the beauti garden\n"
          ]
        }
      ],
      "source": [
        "# Stemming a sentence\n",
        "sentence = \"The runners were running quickly and beautifully through the beautiful garden\"\n",
        "tokens = word_tokenize(sentence.lower())\n",
        "\n",
        "porter = PorterStemmer()\n",
        "stemmed_tokens = [porter.stem(token) for token in tokens]\n",
        "\n",
        "print(\"Original sentence:\", sentence)\n",
        "print(\"\\nOriginal tokens:\", tokens)\n",
        "print(\"Stemmed tokens:\", stemmed_tokens)\n",
        "print(\"\\nStemmed sentence:\", \" \".join(stemmed_tokens))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lemmatization\n",
        "\n",
        "**Lemmatization** is the process of reducing words to their base or dictionary form (lemma). Unlike stemming, lemmatization considers the context and part of speech of the word.\n",
        "\n",
        "## Key Differences from Stemming\n",
        "\n",
        "- **Stemming**: Fast but crude, may produce non-words\n",
        "- **Lemmatization**: Slower but accurate, produces valid words\n",
        "- **Example**: \"better\" â†’ stem: \"better\", lemma: \"good\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lemmatization Examples:\n",
            "------------------------------------------------------------\n",
            "Word            POS   Lemma          \n",
            "------------------------------------------------------------\n",
            "better          a     good           \n",
            "running         v     run            \n",
            "running         n     running        \n",
            "mice            n     mouse          \n",
            "was             v     be             \n",
            "happily         r     happily        \n",
            "happiness       n     happiness      \n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "# Initialize lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Example words with different parts of speech\n",
        "words_examples = [\n",
        "    (\"better\", 'a'),  # adjective\n",
        "    (\"running\", 'v'),  # verb\n",
        "    (\"running\", 'n'),  # noun\n",
        "    (\"mice\", 'n'),  # noun (plural)\n",
        "    (\"was\", 'v'),  # verb\n",
        "    (\"happily\", 'r'),  # adverb\n",
        "    (\"happiness\", 'n')  # noun\n",
        "]\n",
        "\n",
        "print(\"Lemmatization Examples:\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"{'Word':15} {'POS':5} {'Lemma':15}\")\n",
        "print(\"-\" * 60)\n",
        "for word, pos in words_examples:\n",
        "    lemma = lemmatizer.lemmatize(word, pos=pos)\n",
        "    print(f\"{word:15} {pos:5} {lemma:15}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Automatic POS Tagging for Lemmatization\n",
        "\n",
        "Converting POS tags to WordNet format for better lemmatization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original sentence: The better runner was running quickly and feeling happier\n",
            "\n",
            "Tokens with POS tags:\n",
            "  The             -> DT\n",
            "  better          -> JJR\n",
            "  runner          -> NN\n",
            "  was             -> VBD\n",
            "  running         -> VBG\n",
            "  quickly         -> RB\n",
            "  and             -> CC\n",
            "  feeling         -> VBG\n",
            "  happier         -> NN\n",
            "\n",
            "Lemmatized tokens: ['The', 'good', 'runner', 'be', 'run', 'quickly', 'and', 'feel', 'happier']\n",
            "Lemmatized sentence: The good runner be run quickly and feel happier\n"
          ]
        }
      ],
      "source": [
        "def get_wordnet_pos(treebank_tag):\n",
        "    \"\"\"Convert treebank POS tag to WordNet POS tag\"\"\"\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN  # default\n",
        "\n",
        "# Example sentence\n",
        "sentence = \"The better runner was running quickly and feeling happier\"\n",
        "tokens = word_tokenize(sentence)\n",
        "\n",
        "# Get POS tags\n",
        "pos_tags = pos_tag(tokens)\n",
        "\n",
        "# Lemmatize with correct POS\n",
        "lemmatized = []\n",
        "for word, pos in pos_tags:\n",
        "    wordnet_pos = get_wordnet_pos(pos)\n",
        "    lemma = lemmatizer.lemmatize(word, pos=wordnet_pos)\n",
        "    lemmatized.append(lemma)\n",
        "\n",
        "print(\"Original sentence:\", sentence)\n",
        "print(\"\\nTokens with POS tags:\")\n",
        "for word, pos in pos_tags:\n",
        "    print(f\"  {word:15} -> {pos}\")\n",
        "print(\"\\nLemmatized tokens:\", lemmatized)\n",
        "print(\"Lemmatized sentence:\", \" \".join(lemmatized))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Named Entity Recognition (NER)\n",
        "\n",
        "**Named Entity Recognition** is the process of identifying and classifying named entities in text into predefined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, and percentages.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sentence: Apple Inc. is located in Cupertino, California.\n",
            "----------------------------------------------------------------------\n",
            "Named Entities:\n",
            "  Apple                          -> PERSON\n",
            "  Inc.                           -> ORGANIZATION\n",
            "  Cupertino                      -> GPE\n",
            "  California                     -> GPE\n",
            "\n",
            "\n",
            "Sentence: Barack Obama was the President of the United States.\n",
            "----------------------------------------------------------------------\n",
            "Named Entities:\n",
            "  Barack                         -> PERSON\n",
            "  Obama                          -> PERSON\n",
            "  United States                  -> GPE\n",
            "\n",
            "\n",
            "Sentence: I visited Paris, France last summer.\n",
            "----------------------------------------------------------------------\n",
            "Named Entities:\n",
            "  Paris                          -> GPE\n",
            "  France                         -> GPE\n",
            "\n",
            "\n",
            "Sentence: The conference will be held on March 15, 2024 at Stanford University.\n",
            "----------------------------------------------------------------------\n",
            "Named Entities:\n",
            "  Stanford University            -> ORGANIZATION\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from nltk import ne_chunk\n",
        "\n",
        "# Example sentences with named entities\n",
        "sentences = [\n",
        "    \"Apple Inc. is located in Cupertino, California.\",\n",
        "    \"Barack Obama was the President of the United States.\",\n",
        "    \"I visited Paris, France last summer.\",\n",
        "    \"The conference will be held on March 15, 2024 at Stanford University.\"\n",
        "]\n",
        "\n",
        "for sentence in sentences:\n",
        "    print(f\"\\nSentence: {sentence}\")\n",
        "    print(\"-\" * 70)\n",
        "    \n",
        "    # Tokenize and POS tag\n",
        "    tokens = word_tokenize(sentence)\n",
        "    pos_tags = pos_tag(tokens)\n",
        "    \n",
        "    # Perform NER\n",
        "    ner_tree = ne_chunk(pos_tags)\n",
        "    \n",
        "    print(\"Named Entities:\")\n",
        "    for subtree in ner_tree:\n",
        "        if hasattr(subtree, 'label'):\n",
        "            entity_name = ' '.join([token for token, pos in subtree.leaves()])\n",
        "            entity_type = subtree.label()\n",
        "            print(f\"  {entity_name:30} -> {entity_type}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing NER Tree Structure\n",
        "\n",
        "Displaying the full NER tree structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original text: The quick brown fox jumps over the lazy dog.\n",
            "\n",
            "Tokenized: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']\n",
            "\n",
            "POS Tags: [('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n",
            "\n",
            "NER Tree Structure:\n",
            "(S\n",
            "  The/DT\n",
            "  quick/JJ\n",
            "  brown/NN\n",
            "  fox/NN\n",
            "  jumps/VBZ\n",
            "  over/IN\n",
            "  the/DT\n",
            "  lazy/JJ\n",
            "  dog/NN\n",
            "  ./.)\n",
            "\n",
            "Extracted Named Entities:\n",
            "  No named entities found in this sentence.\n"
          ]
        }
      ],
      "source": [
        "# Detailed NER example\n",
        "text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "pos_tags = pos_tag(tokens)\n",
        "ner_tree = ne_chunk(pos_tags)\n",
        "\n",
        "print(\"Original text:\", text)\n",
        "print(\"\\nTokenized:\", tokens)\n",
        "print(\"\\nPOS Tags:\", pos_tags)\n",
        "print(\"\\nNER Tree Structure:\")\n",
        "print(ner_tree)\n",
        "\n",
        "# Extract named entities\n",
        "print(\"\\nExtracted Named Entities:\")\n",
        "entities = []\n",
        "for subtree in ner_tree:\n",
        "    if hasattr(subtree, 'label'):\n",
        "        entity = ' '.join([token for token, pos in subtree.leaves()])\n",
        "        entities.append((entity, subtree.label()))\n",
        "\n",
        "if entities:\n",
        "    for entity, label in entities:\n",
        "        print(f\"  {entity} -> {label}\")\n",
        "else:\n",
        "    print(\"  No named entities found in this sentence.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Text Preprocessing\n",
        "\n",
        "**Text Preprocessing** is a crucial step in NLP that involves cleaning and preparing text data for analysis. Common preprocessing steps include:\n",
        "\n",
        "1. **Lowercasing**: Convert all text to lowercase\n",
        "2. **Removing Punctuation**: Remove special characters\n",
        "3. **Removing Stopwords**: Remove common words that don't carry much meaning\n",
        "4. **Tokenization**: Split text into tokens\n",
        "5. **Stemming/Lemmatization**: Reduce words to their base forms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original text:\n",
            "The quick brown fox jumps over the lazy dog! It's a beautiful day in the neighborhood.\n",
            "\n",
            "======================================================================\n",
            "\n",
            "1. After lowercasing:\n",
            "the quick brown fox jumps over the lazy dog! it's a beautiful day in the neighborhood.\n",
            "\n",
            "2. After tokenization:\n",
            "['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '!', 'it', \"'s\", 'a', 'beautiful', 'day', 'in', 'the', 'neighborhood', '.']\n",
            "\n",
            "3. After removing punctuation:\n",
            "['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', 'it', \"'s\", 'a', 'beautiful', 'day', 'in', 'the', 'neighborhood']\n",
            "\n",
            "4. After removing stopwords:\n",
            "['quick', 'brown', 'fox', 'jumps', 'lazy', 'dog', \"'s\", 'beautiful', 'day', 'neighborhood']\n",
            "Removed stopwords: ['the', 'over', 'the', 'it', 'a', 'in', 'the']\n",
            "\n",
            "5. After stemming:\n",
            "['quick', 'brown', 'fox', 'jump', 'lazi', 'dog', \"'s\", 'beauti', 'day', 'neighborhood']\n",
            "\n",
            "======================================================================\n",
            "Final preprocessed text: quick brown fox jump lazi dog 's beauti day neighborhood\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Example text\n",
        "text = \"The quick brown fox jumps over the lazy dog! It's a beautiful day in the neighborhood.\"\n",
        "\n",
        "print(\"Original text:\")\n",
        "print(text)\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "# Step 1: Lowercasing\n",
        "text_lower = text.lower()\n",
        "print(\"\\n1. After lowercasing:\")\n",
        "print(text_lower)\n",
        "\n",
        "# Step 2: Tokenization\n",
        "tokens = word_tokenize(text_lower)\n",
        "print(\"\\n2. After tokenization:\")\n",
        "print(tokens)\n",
        "\n",
        "# Step 3: Removing punctuation\n",
        "tokens_no_punct = [token for token in tokens if token not in string.punctuation]\n",
        "print(\"\\n3. After removing punctuation:\")\n",
        "print(tokens_no_punct)\n",
        "\n",
        "# Step 4: Removing stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens_no_stopwords = [token for token in tokens_no_punct if token not in stop_words]\n",
        "print(\"\\n4. After removing stopwords:\")\n",
        "print(tokens_no_stopwords)\n",
        "print(f\"Removed stopwords: {[w for w in tokens_no_punct if w in stop_words]}\")\n",
        "\n",
        "# Step 5: Stemming (optional)\n",
        "porter = PorterStemmer()\n",
        "tokens_stemmed = [porter.stem(token) for token in tokens_no_stopwords]\n",
        "print(\"\\n5. After stemming:\")\n",
        "print(tokens_stemmed)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Final preprocessed text:\", \" \".join(tokens_stemmed))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Complete Preprocessing Function\n",
        "\n",
        "Creating a reusable preprocessing function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text Preprocessing Examples:\n",
            "======================================================================\n",
            "\n",
            "Original: The quick brown fox jumps over the lazy dog!\n",
            "Preprocessed: quick brown fox jump lazi dog\n",
            "\n",
            "Original: Natural Language Processing is amazing and powerful.\n",
            "Preprocessed: natur languag process amaz power\n",
            "\n",
            "Original: I'm learning about text preprocessing techniques.\n",
            "Preprocessed: 'm learn text preprocess techniqu\n"
          ]
        }
      ],
      "source": [
        "def preprocess_text(text, remove_stopwords=True, stem=True, lemmatize=False):\n",
        "    \"\"\"\n",
        "    Complete text preprocessing pipeline\n",
        "    \n",
        "    Parameters:\n",
        "    - text: Input text string\n",
        "    - remove_stopwords: Whether to remove stopwords\n",
        "    - stem: Whether to apply stemming\n",
        "    - lemmatize: Whether to apply lemmatization (overrides stem if True)\n",
        "    \n",
        "    Returns:\n",
        "    - List of preprocessed tokens\n",
        "    \"\"\"\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "    \n",
        "    # Remove punctuation\n",
        "    tokens = [token for token in tokens if token not in string.punctuation]\n",
        "    \n",
        "    # Remove stopwords\n",
        "    if remove_stopwords:\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        tokens = [token for token in tokens if token not in stop_words]\n",
        "    \n",
        "    # Lemmatization or Stemming\n",
        "    if lemmatize:\n",
        "        pos_tags = pos_tag(tokens)\n",
        "        lemmatized = []\n",
        "        for word, pos in pos_tags:\n",
        "            wordnet_pos = get_wordnet_pos(pos)\n",
        "            lemma = lemmatizer.lemmatize(word, pos=wordnet_pos)\n",
        "            lemmatized.append(lemma)\n",
        "        tokens = lemmatized\n",
        "    elif stem:\n",
        "        porter = PorterStemmer()\n",
        "        tokens = [porter.stem(token) for token in tokens]\n",
        "    \n",
        "    return tokens\n",
        "\n",
        "# Test the function\n",
        "sample_texts = [\n",
        "    \"The quick brown fox jumps over the lazy dog!\",\n",
        "    \"Natural Language Processing is amazing and powerful.\",\n",
        "    \"I'm learning about text preprocessing techniques.\"\n",
        "]\n",
        "\n",
        "print(\"Text Preprocessing Examples:\")\n",
        "print(\"=\"*70)\n",
        "for text in sample_texts:\n",
        "    print(f\"\\nOriginal: {text}\")\n",
        "    preprocessed = preprocess_text(text, remove_stopwords=True, stem=True)\n",
        "    print(f\"Preprocessed: {' '.join(preprocessed)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bag of Words (BoW)\n",
        "\n",
        "**Bag of Words** is a simple text representation method that creates a vocabulary of unique words and represents each document as a vector of word counts, ignoring word order and grammar.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Documents:\n",
            "1. I love natural language processing\n",
            "2. Natural language processing is fascinating\n",
            "3. I love machine learning and NLP\n",
            "4. Machine learning and natural language processing are related\n",
            "\n",
            "Vocabulary: ['and' 'are' 'fascinating' 'is' 'language' 'learning' 'love' 'machine'\n",
            " 'natural' 'nlp' 'processing' 'related']\n",
            "\n",
            "Vocabulary size: 12\n",
            "\n",
            "BoW Matrix shape: (4, 12)\n",
            "\n",
            "Bag of Words Matrix:\n",
            "       and  are  fascinating  is  language  learning  love  machine  natural  \\\n",
            "Doc 1    0    0            0   0         1         0     1        0        1   \n",
            "Doc 2    0    0            1   1         1         0     0        0        1   \n",
            "Doc 3    1    0            0   0         0         1     1        1        0   \n",
            "Doc 4    1    1            0   0         1         1     0        1        1   \n",
            "\n",
            "       nlp  processing  related  \n",
            "Doc 1    0           1        0  \n",
            "Doc 2    0           1        0  \n",
            "Doc 3    1           0        0  \n",
            "Doc 4    0           1        1  \n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "# Sample documents\n",
        "documents = [\n",
        "    \"I love natural language processing\",\n",
        "    \"Natural language processing is fascinating\",\n",
        "    \"I love machine learning and NLP\",\n",
        "    \"Machine learning and natural language processing are related\"\n",
        "]\n",
        "\n",
        "# Create CountVectorizer (Bag of Words)\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit and transform documents\n",
        "bow_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Get vocabulary\n",
        "vocabulary = vectorizer.get_feature_names_out()\n",
        "\n",
        "print(\"Documents:\")\n",
        "for i, doc in enumerate(documents, 1):\n",
        "    print(f\"{i}. {doc}\")\n",
        "\n",
        "print(\"\\nVocabulary:\", vocabulary)\n",
        "print(f\"\\nVocabulary size: {len(vocabulary)}\")\n",
        "print(f\"\\nBoW Matrix shape: {bow_matrix.shape}\")\n",
        "\n",
        "# Convert to dense array for better visualization\n",
        "bow_dense = bow_matrix.toarray()\n",
        "\n",
        "# Create DataFrame for better visualization\n",
        "df_bow = pd.DataFrame(bow_dense, columns=vocabulary, index=[f\"Doc {i+1}\" for i in range(len(documents))])\n",
        "print(\"\\nBag of Words Matrix:\")\n",
        "print(df_bow)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bag of Words with Custom Parameters\n",
        "\n",
        "Using custom parameters like max_features, min_df, and max_df.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Custom BoW with preprocessing:\n",
            "Vocabulary: ['fascinating' 'language' 'learning' 'love' 'machine' 'natural' 'nlp'\n",
            " 'processing' 'related']\n",
            "\n",
            "BoW Matrix:\n",
            "       fascinating  language  learning  love  machine  natural  nlp  \\\n",
            "Doc 1            0         1         0     1        0        1    0   \n",
            "Doc 2            1         1         0     0        0        1    0   \n",
            "Doc 3            0         0         1     1        1        0    1   \n",
            "Doc 4            0         1         1     0        1        1    0   \n",
            "\n",
            "       processing  related  \n",
            "Doc 1           1        0  \n",
            "Doc 2           1        0  \n",
            "Doc 3           0        0  \n",
            "Doc 4           1        1  \n"
          ]
        }
      ],
      "source": [
        "# Custom Bag of Words with preprocessing\n",
        "vectorizer_custom = CountVectorizer(\n",
        "    max_features=10,  # Keep only top 10 most frequent words\n",
        "    min_df=1,  # Minimum document frequency\n",
        "    max_df=0.8,  # Maximum document frequency (ignore words in >80% of docs)\n",
        "    stop_words='english',  # Remove English stopwords\n",
        "    lowercase=True,\n",
        "    token_pattern=r'\\b\\w+\\b'  # Word token pattern\n",
        ")\n",
        "\n",
        "bow_custom = vectorizer_custom.fit_transform(documents)\n",
        "vocab_custom = vectorizer_custom.get_feature_names_out()\n",
        "\n",
        "print(\"Custom BoW with preprocessing:\")\n",
        "print(f\"Vocabulary: {vocab_custom}\")\n",
        "print(f\"\\nBoW Matrix:\")\n",
        "df_custom = pd.DataFrame(bow_custom.toarray(), columns=vocab_custom, \n",
        "                         index=[f\"Doc {i+1}\" for i in range(len(documents))])\n",
        "print(df_custom)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TF-IDF (Term Frequency-Inverse Document Frequency)\n",
        "\n",
        "**TF-IDF** is a numerical statistic that reflects how important a word is to a document in a collection of documents. It increases proportionally to the number of times a word appears in a document but is offset by the frequency of the word in the corpus.\n",
        "\n",
        "## Formula\n",
        "\n",
        "- **TF (Term Frequency)**: Number of times a term appears in a document\n",
        "- **IDF (Inverse Document Frequency)**: Logarithmic inverse of the document frequency\n",
        "- **TF-IDF = TF Ã— IDF**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Documents:\n",
            "1. I love natural language processing\n",
            "2. Natural language processing is fascinating\n",
            "3. I love machine learning and NLP\n",
            "4. Machine learning and natural language processing are related\n",
            "\n",
            "Vocabulary size: 12\n",
            "TF-IDF Matrix shape: (4, 12)\n",
            "\n",
            "TF-IDF Matrix:\n",
            "         and    are  fascinating     is  language  learning   love  machine  \\\n",
            "Doc 1  0.000  0.000        0.000  0.000     0.470     0.000  0.581    0.000   \n",
            "Doc 2  0.000  0.000        0.557  0.557     0.356     0.000  0.000    0.000   \n",
            "Doc 3  0.422  0.000        0.000  0.000     0.000     0.422  0.422    0.422   \n",
            "Doc 4  0.350  0.443        0.000  0.000     0.283     0.350  0.000    0.350   \n",
            "\n",
            "       natural    nlp  processing  related  \n",
            "Doc 1    0.470  0.000       0.470    0.000  \n",
            "Doc 2    0.356  0.000       0.356    0.000  \n",
            "Doc 3    0.000  0.536       0.000    0.000  \n",
            "Doc 4    0.283  0.000       0.283    0.443  \n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Same documents as before\n",
        "documents = [\n",
        "    \"I love natural language processing\",\n",
        "    \"Natural language processing is fascinating\",\n",
        "    \"I love machine learning and NLP\",\n",
        "    \"Machine learning and natural language processing are related\"\n",
        "]\n",
        "\n",
        "# Create TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform documents\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
        "\n",
        "# Get vocabulary\n",
        "vocabulary = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "print(\"Documents:\")\n",
        "for i, doc in enumerate(documents, 1):\n",
        "    print(f\"{i}. {doc}\")\n",
        "\n",
        "print(f\"\\nVocabulary size: {len(vocabulary)}\")\n",
        "print(f\"TF-IDF Matrix shape: {tfidf_matrix.shape}\")\n",
        "\n",
        "# Convert to dense array\n",
        "tfidf_dense = tfidf_matrix.toarray()\n",
        "\n",
        "# Create DataFrame\n",
        "df_tfidf = pd.DataFrame(tfidf_dense, columns=vocabulary, \n",
        "                        index=[f\"Doc {i+1}\" for i in range(len(documents))])\n",
        "print(\"\\nTF-IDF Matrix:\")\n",
        "print(df_tfidf.round(3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding TF-IDF Values\n",
        "\n",
        "Analyzing which words have high TF-IDF scores in each document.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Document 1: I love natural language processing\n",
            "------------------------------------------------------------\n",
            "Top 5 words by TF-IDF score:\n",
            "  love                 -> 0.5806\n",
            "  processing           -> 0.4701\n",
            "  natural              -> 0.4701\n",
            "  language             -> 0.4701\n",
            "  related              -> 0.0000\n",
            "\n",
            "Document 2: Natural language processing is fascinating\n",
            "------------------------------------------------------------\n",
            "Top 5 words by TF-IDF score:\n",
            "  is                   -> 0.5571\n",
            "  fascinating          -> 0.5571\n",
            "  processing           -> 0.3556\n",
            "  natural              -> 0.3556\n",
            "  language             -> 0.3556\n",
            "\n",
            "Document 3: I love machine learning and NLP\n",
            "------------------------------------------------------------\n",
            "Top 5 words by TF-IDF score:\n",
            "  nlp                  -> 0.5356\n",
            "  machine              -> 0.4222\n",
            "  love                 -> 0.4222\n",
            "  learning             -> 0.4222\n",
            "  and                  -> 0.4222\n",
            "\n",
            "Document 4: Machine learning and natural language processing are related\n",
            "------------------------------------------------------------\n",
            "Top 5 words by TF-IDF score:\n",
            "  related              -> 0.4434\n",
            "  are                  -> 0.4434\n",
            "  machine              -> 0.3496\n",
            "  learning             -> 0.3496\n",
            "  and                  -> 0.3496\n"
          ]
        }
      ],
      "source": [
        "# Find top words for each document\n",
        "for i, doc in enumerate(documents):\n",
        "    print(f\"\\nDocument {i+1}: {doc}\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    # Get TF-IDF scores for this document\n",
        "    scores = tfidf_dense[i]\n",
        "    \n",
        "    # Get top 5 words\n",
        "    top_indices = scores.argsort()[-5:][::-1]\n",
        "    \n",
        "    print(\"Top 5 words by TF-IDF score:\")\n",
        "    for idx in top_indices:\n",
        "        word = vocabulary[idx]\n",
        "        score = scores[idx]\n",
        "        print(f\"  {word:20} -> {score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TF-IDF with Custom Parameters\n",
        "\n",
        "Using n-grams and custom parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF-IDF with Bigrams:\n",
            "\n",
            "Vocabulary (first 20 features): ['fascinating' 'language' 'language processing' 'learning'\n",
            " 'learning natural' 'learning nlp' 'love' 'love machine' 'love natural'\n",
            " 'machine' 'machine learning' 'natural' 'natural language' 'nlp'\n",
            " 'processing' 'processing fascinating' 'processing related' 'related']\n",
            "\n",
            "Matrix shape: (4, 18)\n",
            "\n",
            "TF-IDF Matrix with Bigrams:\n",
            "       fascinating  language  language processing  learning  learning natural  \\\n",
            "Doc 1        0.000     0.334                0.334     0.000             0.000   \n",
            "Doc 2        0.498     0.318                0.318     0.000             0.000   \n",
            "Doc 3        0.000     0.000                0.000     0.337             0.000   \n",
            "Doc 4        0.000     0.243                0.243     0.300             0.381   \n",
            "\n",
            "       learning nlp   love  love machine  love natural  machine  \\\n",
            "Doc 1         0.000  0.412         0.000         0.523    0.000   \n",
            "Doc 2         0.000  0.000         0.000         0.000    0.000   \n",
            "Doc 3         0.427  0.337         0.427         0.000    0.337   \n",
            "Doc 4         0.000  0.000         0.000         0.000    0.300   \n",
            "\n",
            "       machine learning  natural  natural language    nlp  processing  \\\n",
            "Doc 1             0.000    0.334             0.334  0.000       0.334   \n",
            "Doc 2             0.000    0.318             0.318  0.000       0.318   \n",
            "Doc 3             0.337    0.000             0.000  0.427       0.000   \n",
            "Doc 4             0.300    0.243             0.243  0.000       0.243   \n",
            "\n",
            "       processing fascinating  processing related  related  \n",
            "Doc 1                   0.000               0.000    0.000  \n",
            "Doc 2                   0.498               0.000    0.000  \n",
            "Doc 3                   0.000               0.000    0.000  \n",
            "Doc 4                   0.000               0.381    0.381  \n"
          ]
        }
      ],
      "source": [
        "# TF-IDF with bigrams and custom parameters\n",
        "tfidf_bigram = TfidfVectorizer(\n",
        "    ngram_range=(1, 2),  # Unigrams and bigrams\n",
        "    max_features=20,  # Top 20 features\n",
        "    stop_words='english',\n",
        "    min_df=1,\n",
        "    max_df=0.8\n",
        ")\n",
        "\n",
        "tfidf_bigram_matrix = tfidf_bigram.fit_transform(documents)\n",
        "vocab_bigram = tfidf_bigram.get_feature_names_out()\n",
        "\n",
        "print(\"TF-IDF with Bigrams:\")\n",
        "print(f\"\\nVocabulary (first 20 features): {vocab_bigram}\")\n",
        "print(f\"\\nMatrix shape: {tfidf_bigram_matrix.shape}\")\n",
        "\n",
        "df_bigram = pd.DataFrame(tfidf_bigram_matrix.toarray(), columns=vocab_bigram,\n",
        "                        index=[f\"Doc {i+1}\" for i in range(len(documents))])\n",
        "print(\"\\nTF-IDF Matrix with Bigrams:\")\n",
        "print(df_bigram.round(3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Word Embeddings\n",
        "\n",
        "**Word Embeddings** are dense vector representations of words in a continuous vector space. Unlike sparse representations (like BoW or TF-IDF), embeddings capture semantic relationships between words.\n",
        "\n",
        "## Types of Word Embeddings\n",
        "\n",
        "1. **Word2Vec**: Predicts words from context (CBOW) or context from words (Skip-gram)\n",
        "2. **GloVe**: Global Vectors for Word Representation using co-occurrence statistics\n",
        "3. **FastText**: Extends Word2Vec with subword information\n",
        "4. **Contextual Embeddings**: BERT, ELMo, GPT (capture context-dependent meanings)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gensim is not installed.\n",
            "\n",
            "To use word embeddings, install gensim:\n",
            "  pip install gensim\n",
            "\n",
            "Alternatively, you can use other embedding libraries like:\n",
            "  - spaCy (spacy.io)\n",
            "  - transformers (Hugging Face)\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import gensim\n",
        "    from gensim.models import Word2Vec\n",
        "    from gensim.downloader import load\n",
        "    \n",
        "    print(\"Gensim is available. Loading pre-trained GloVe embeddings...\")\n",
        "    \n",
        "    # Load pre-trained GloVe embeddings (smaller model for demonstration)\n",
        "    # This will download the model if not already present\n",
        "    try:\n",
        "        glove_model = load('glove-wiki-gigaword-50')\n",
        "        print(\"âœ“ GloVe model loaded successfully!\")\n",
        "        print(f\"\\nVocabulary size: {len(glove_model.key_to_index)}\")\n",
        "        print(f\"Vector dimensions: {glove_model.vector_size}\")\n",
        "        \n",
        "        # Example: Get word vector\n",
        "        word = \"king\"\n",
        "        if word in glove_model:\n",
        "            vector = glove_model[word]\n",
        "            print(f\"\\nVector for '{word}' (first 10 dimensions): {vector[:10]}\")\n",
        "            print(f\"Vector shape: {vector.shape}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error loading GloVe model: {e}\")\n",
        "        print(\"\\nNote: You can install gensim with: pip install gensim\")\n",
        "        \n",
        "except ImportError:\n",
        "    print(\"Gensim is not installed.\")\n",
        "    print(\"\\nTo use word embeddings, install gensim:\")\n",
        "    print(\"  pip install gensim\")\n",
        "    print(\"\\nAlternatively, you can use other embedding libraries like:\")\n",
        "    print(\"  - spaCy (spacy.io)\")\n",
        "    print(\"  - transformers (Hugging Face)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Finding Similar Words\n",
        "\n",
        "Using word embeddings to find semantically similar words.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GloVe model not loaded. Please run the previous cell first.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    if 'glove_model' in locals():\n",
        "        # Find most similar words\n",
        "        test_words = [\"king\", \"computer\", \"beautiful\", \"happy\"]\n",
        "        \n",
        "        for word in test_words:\n",
        "            if word in glove_model:\n",
        "                print(f\"\\nWords similar to '{word}':\")\n",
        "                similar = glove_model.most_similar(word, topn=5)\n",
        "                for similar_word, score in similar:\n",
        "                    print(f\"  {similar_word:20} (similarity: {score:.4f})\")\n",
        "            else:\n",
        "                print(f\"'{word}' not found in vocabulary\")\n",
        "    else:\n",
        "        print(\"GloVe model not loaded. Please run the previous cell first.\")\n",
        "except NameError:\n",
        "    print(\"GloVe model not available. Please install gensim and run the previous cell.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Word Analogies\n",
        "\n",
        "Demonstrating word relationships using embeddings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GloVe model not available. Please install gensim and run the previous cell.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    if 'glove_model' in locals():\n",
        "        # Word analogy: king - man + woman = ?\n",
        "        # This should give us \"queen\"\n",
        "        analogy_words = [\"king\", \"man\", \"woman\"]\n",
        "        \n",
        "        if all(word in glove_model for word in analogy_words):\n",
        "            result = glove_model.most_similar(positive=[\"king\", \"woman\"], \n",
        "                                           negative=[\"man\"], topn=5)\n",
        "            \n",
        "            print(\"Word Analogy: king - man + woman = ?\")\n",
        "            print(\"-\" * 50)\n",
        "            for word, score in result:\n",
        "                print(f\"  {word:20} (score: {score:.4f})\")\n",
        "            \n",
        "            print(\"\\n\" + \"=\"*50)\n",
        "            \n",
        "            # Another analogy: computer - machine + human = ?\n",
        "            analogy_words2 = [\"computer\", \"machine\", \"human\"]\n",
        "            if all(word in glove_model for word in analogy_words2):\n",
        "                result2 = glove_model.most_similar(positive=[\"computer\", \"human\"], \n",
        "                                                 negative=[\"machine\"], topn=5)\n",
        "                \n",
        "                print(\"Word Analogy: computer - machine + human = ?\")\n",
        "                print(\"-\" * 50)\n",
        "                for word, score in result2:\n",
        "                    print(f\"  {word:20} (score: {score:.4f})\")\n",
        "        else:\n",
        "            print(\"Some words not found in vocabulary\")\n",
        "    else:\n",
        "        print(\"GloVe model not available. Please install gensim and run the previous cell.\")\n",
        "except NameError:\n",
        "    print(\"GloVe model not available. Please install gensim and run the previous cell.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Custom Word2Vec Model\n",
        "\n",
        "Creating a simple Word2Vec model from your own text data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gensim is not installed. Install it with: pip install gensim\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    from gensim.models import Word2Vec\n",
        "    \n",
        "    # Sample sentences (tokenized)\n",
        "    sentences = [\n",
        "        [\"natural\", \"language\", \"processing\", \"is\", \"fascinating\"],\n",
        "        [\"machine\", \"learning\", \"and\", \"nlp\", \"are\", \"related\"],\n",
        "        [\"word\", \"embeddings\", \"capture\", \"semantic\", \"relationships\"],\n",
        "        [\"text\", \"preprocessing\", \"is\", \"important\", \"for\", \"nlp\"],\n",
        "        [\"tokenization\", \"stemming\", \"and\", \"lemmatization\", \"are\", \"techniques\"],\n",
        "        [\"named\", \"entity\", \"recognition\", \"identifies\", \"entities\"],\n",
        "        [\"bag\", \"of\", \"words\", \"represents\", \"text\", \"as\", \"vectors\"],\n",
        "        [\"tf\", \"idf\", \"weights\", \"words\", \"by\", \"importance\"]\n",
        "    ]\n",
        "    \n",
        "    # Train Word2Vec model\n",
        "    model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "    \n",
        "    print(\"Custom Word2Vec Model Trained!\")\n",
        "    print(f\"Vocabulary size: {len(model.wv.key_to_index)}\")\n",
        "    print(f\"Vector dimensions: {model.wv.vector_size}\")\n",
        "    \n",
        "    # Find similar words\n",
        "    if \"nlp\" in model.wv:\n",
        "        print(\"\\nWords similar to 'nlp':\")\n",
        "        similar = model.wv.most_similar(\"nlp\", topn=5)\n",
        "        for word, score in similar:\n",
        "            print(f\"  {word:20} (similarity: {score:.4f})\")\n",
        "    \n",
        "    if \"word\" in model.wv:\n",
        "        print(\"\\nWords similar to 'word':\")\n",
        "        similar = model.wv.most_similar(\"word\", topn=5)\n",
        "        for word, score in similar:\n",
        "            print(f\"  {word:20} (similarity: {score:.4f})\")\n",
        "            \n",
        "except ImportError:\n",
        "    print(\"Gensim is not installed. Install it with: pip install gensim\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Summary\n",
        "\n",
        "This notebook covered:\n",
        "\n",
        "1. **Tokenization**: Breaking text into sentences and words\n",
        "2. **Stemming**: Reducing words to their root forms\n",
        "3. **Lemmatization**: More accurate word reduction using POS tags\n",
        "4. **Named Entity Recognition**: Identifying entities in text\n",
        "5. **Text Preprocessing**: Complete pipeline for cleaning text\n",
        "6. **Bag of Words**: Simple word count representation\n",
        "7. **TF-IDF**: Weighted word importance representation\n",
        "8. **Word Embeddings**: Dense vector representations capturing semantics\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "- Explore transformer models (BERT, GPT)\n",
        "- Experiment with spaCy for advanced NLP\n",
        "- Build text classification models\n",
        "- Work with sequence models (RNNs, LSTMs, Transformers)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
