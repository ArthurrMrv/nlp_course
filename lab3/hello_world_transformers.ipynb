{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hello world Transformers\n",
        "\n",
        "This notebook explores the basics of Hugging Face Transformers using pre-trained models to perform various NLP tasks. We'll use pipelines to classify text, recognize named entities, answer questions, summarize text, translate, and generate text.\n",
        "\n",
        "**⚠️ Do not forget to install the transformers library to run this notebook.**\n",
        "\n",
        "## Quick overview of Transformer applications"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"\"\"Dear Amazon, last week I ordered an Optimus Prime action figure from your online store in Germany. However, when the package arrived, I discovered to my horror that you had sent me a Megatron action figure instead. As a lifelong fan of the Transformers franchise, I cannot express how disappointed I am. Optimus Prime is the noble leader of the Autobots, while Megatron is the treacherous leader of the Decepticons. I specifically ordered Optimus Prime, and receiving Megatron instead is simply unacceptable. I demand a full refund and a replacement with the correct action figure. I hope Bumblebee can help resolve this matter quickly.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 1: Understanding Pipelines\n",
        "\n",
        "1. **What is a `pipeline` in Hugging Face Transformers?**  \n",
        "   A `pipeline` is a high-level abstraction that combines a model, tokenizer, and preprocessing/postprocessing steps into a single, easy-to-use interface. It abstracts away:\n",
        "   - Model loading and initialization\n",
        "   - Tokenization (converting text to tokens)\n",
        "   - Model inference (running the model)\n",
        "   - Post-processing (converting model outputs to human-readable format)\n",
        "   - Device management (CPU/GPU/MPS)\n",
        "   - Batch processing\n",
        "\n",
        "2. **Other tasks besides text-classification:**  \n",
        "   - `\"sentiment-analysis\"` (similar to text-classification)\n",
        "   - `\"ner\"` (Named Entity Recognition)\n",
        "   - `\"question-answering\"` or `\"qa\"`\n",
        "   - `\"summarization\"`\n",
        "   - `\"translation\"` or `\"translation_en_to_fr\"` (language-specific)\n",
        "   - `\"text-generation\"`\n",
        "   - `\"zero-shot-classification\"`\n",
        "   - `\"fill-mask\"`\n",
        "   - `\"feature-extraction\"`\n",
        "   - `\"conversational\"`\n",
        "\n",
        "3. **What happens when you don't specify a model?**  \n",
        "   When no model is specified, Hugging Face uses a default pre-trained model for that task. The pipeline automatically selects a suitable model from the Hub (usually a popular, well-performing model). You can specify a model by passing the `model` parameter:\n",
        "   ```python\n",
        "   classifier = pipeline(\"text-classification\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "   ```\n",
        "   Or use a model identifier from the Hub:\n",
        "   ```python\n",
        "   classifier = pipeline(\"text-classification\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
        "   ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"text-classification\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 2: Text Classification Deep Dive\n",
        "\n",
        "1. **Default model used:**  \n",
        "   The default model is typically `distilbert-base-uncased-finetuned-sst-2-english`. This is a DistilBERT model (a smaller, faster version of BERT) that has been fine-tuned on the SST-2 (Stanford Sentiment Treebank v2) dataset.\n",
        "\n",
        "2. **Dataset and text type:**  \n",
        "   - **Dataset:** SST-2 (Stanford Sentiment Treebank v2) - a binary sentiment classification dataset with movie reviews\n",
        "   - **Works best with:** English text, particularly reviews, opinions, and subjective text. It's trained on movie reviews, so it performs well on similar opinionated text.\n",
        "\n",
        "3. **What does `score` represent?**  \n",
        "   The `score` represents the model's confidence/probability for the predicted label. It's a value between 0 and 1, where:\n",
        "   - Values closer to 1 indicate higher confidence\n",
        "   - Values closer to 0.5 indicate uncertainty\n",
        "   - The scores for all labels typically sum to 1 (softmax probabilities)\n",
        "\n",
        "4. **Emotion classification model:**  \n",
        "   One example is `j-hartmann/emotion-english-distilroberta-base` which classifies text into 7 emotions: anger, disgust, fear, joy, neutral, sadness, and surprise. Another popular one is `bhadresh-savani/bert-base-uncased-emotion` which classifies into 6 emotions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "outputs = classifier(text)\n",
        "pd.DataFrame(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Named Entity Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 3: Named Entity Recognition (NER)\n",
        "\n",
        "1. **What does `aggregation_strategy=\"simple\"` do?**  \n",
        "   `aggregation_strategy=\"simple\"` groups subword tokens that belong to the same entity into a single entity. For example, if \"Optimus\" and \"##Prime\" are both tagged as MISC, they get combined into \"Optimus Prime\" as one entity. Without this, you'd see fragmented entities split across multiple tokens.\n",
        "\n",
        "2. **Entity types meaning:**  \n",
        "   - **ORG** (Organization): Companies, institutions, groups (e.g., \"Amazon\", \"Autobots\")\n",
        "   - **MISC** (Miscellaneous): Other named entities that don't fit other categories (e.g., \"Optimus Prime\", product names)\n",
        "   - **LOC** (Location): Geographic locations (e.g., \"Germany\", \"New York\")\n",
        "   - **PER** (Person): Person names (e.g., \"Bumblebee\" - though this is actually a character name)\n",
        "\n",
        "3. **Why `##` prefix?**  \n",
        "   The `##` prefix indicates that this token is a **subword continuation** from WordPiece/BPE tokenization. When a word is split into multiple tokens, the first token keeps the original form, and subsequent tokens get the `##` prefix. For example:\n",
        "   - \"Megatron\" → [\"Mega\", \"##tron\"]\n",
        "   - \"Decepticons\" → [\"Decept\", \"##icons\"]\n",
        "   This is how tokenizers handle out-of-vocabulary words by breaking them into smaller pieces.\n",
        "\n",
        "4. **Why \"Megatron\" and \"Decepticons\" split incorrectly?**  \n",
        "   These are fictional character names from Transformers that likely weren't in the model's training vocabulary. The model was trained on CoNLL-2003, which contains real-world entities (news articles). Since these fictional names are rare/absent, the tokenizer splits them into subwords, and the NER model may not recognize them as complete entities. This tells us the model's training data domain (news) doesn't include fictional/fantasy content.\n",
        "\n",
        "5. **CoNLL-2003 dataset:**  \n",
        "   CoNLL-2003 is a Named Entity Recognition dataset from the Conference on Computational Natural Language Learning. It contains English news articles annotated with four entity types: PER, LOC, ORG, and MISC. The dataset is split into training, validation, and test sets. Tokenizer choice affects NER because:\n",
        "   - Better tokenization preserves entity boundaries\n",
        "   - Subword tokenization can fragment entities\n",
        "   - Case-sensitive tokenizers (like `bert-large-cased`) preserve capitalization cues important for NER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
        "outputs = ner_tagger(text)\n",
        "pd.DataFrame(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 4: Question Answering Systems\n",
        "\n",
        "1. **Type of QA:**  \n",
        "   This is **Extractive Question Answering**. The model extracts a span of text directly from the input context that answers the question, rather than generating a new answer.\n",
        "\n",
        "2. **What do start/end indices represent?**  \n",
        "   The `start` and `end` indices indicate the character positions in the original context text where the answer begins and ends. They're important because:\n",
        "   - They allow you to extract the exact answer substring from the original text\n",
        "   - They enable highlighting/visualization of answers\n",
        "   - They provide interpretability (you can see exactly what text the model selected)\n",
        "   - They're used for evaluation metrics (exact match, F1 score)\n",
        "\n",
        "3. **What is the SQuAD dataset?**  \n",
        "   SQuAD (Stanford Question Answering Dataset) is a reading comprehension dataset consisting of questions posed by crowdworkers on a set of Wikipedia articles. The answer to every question is a segment of text (span) from the corresponding reading passage. SQuAD 1.1 has 100,000+ question-answer pairs. SQuAD 2.0 includes unanswerable questions.\n",
        "\n",
        "4. **Questions the model cannot answer:**  \n",
        "   - Questions requiring information not in the text: \"What is the customer's email address?\"\n",
        "   - Questions requiring reasoning beyond the text: \"Why is the customer more upset about receiving Megatron than Optimus Prime?\" (requires knowledge about Transformers lore)\n",
        "   - Questions requiring numerical calculations: \"How many days ago did the customer order?\"\n",
        "   - Questions about future events or hypotheticals: \"What will Amazon do next?\"\n",
        "   The model fails because it can only extract spans from the given context and cannot perform external reasoning or access world knowledge.\n",
        "\n",
        "5. **Extractive vs Generative QA:**  \n",
        "   - **Extractive QA:** Selects a span from the context (faster, more accurate for factual questions, but limited to what's in the text)\n",
        "   - **Generative QA:** Generates new text as an answer (can synthesize information, answer questions not directly in text, but may hallucinate)\n",
        "   \n",
        "   **Example generative QA model:** `google/flan-t5-base` or `google/flan-t5-large` can be used for generative QA. Models like `microsoft/DialoGPT-medium` or `facebook/blenderbot-400M-distill` are conversational models that can generate answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question Answering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reader = pipeline(\"question-answering\")\n",
        "\n",
        "question = \"What does the customer want?\"\n",
        "outputs = reader(question=question, context=text)\n",
        "\n",
        "pd.DataFrame([outputs])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 5: Text Summarization\n",
        "\n",
        "1. **Extractive vs Abstractive summarization:**  \n",
        "   - **Extractive:** Selects and concatenates important sentences/phrases directly from the source text (like highlighting key sentences). Preserves original wording.\n",
        "   - **Abstractive:** Generates new sentences that capture the meaning, potentially using different words than the source (like writing a summary in your own words). More flexible but harder.\n",
        "\n",
        "2. **Default model:**  \n",
        "   The default is typically `sshleifer/distilbart-cnn-12-6` or similar BART-based model. This is an **abstractive** summarization model using the **BART architecture** (encoder-decoder transformer, similar to GPT for decoder, BERT for encoder). It's trained on **CNN/DailyMail** dataset (news articles with summaries).\n",
        "\n",
        "3. **`max_length` and `min_length`:**  \n",
        "   - `max_length`: Maximum number of tokens in the generated summary\n",
        "   - `min_length`: Minimum number of tokens in the generated summary\n",
        "   - If `min_length > max_length`: The model will generate up to `max_length` tokens and ignore the `min_length` constraint (you'll see a warning). This is what happens in the code cell above!\n",
        "\n",
        "4. **`clean_up_tokenization_spaces=True`:**  \n",
        "   This parameter removes extra spaces that can be introduced during tokenization. For example, tokenizers might add spaces around punctuation or split words in ways that create awkward spacing. Setting this to `True` produces cleaner, more readable output by normalizing whitespace.\n",
        "\n",
        "5. **Two summarization models comparison:**  \n",
        "   - **For short texts (news):** `facebook/bart-large-cnn` - BART architecture, trained on CNN/DailyMail, good for news articles (typically 500-1000 words)\n",
        "   - **For longer documents:** `google/pegasus-xsum` or `google/pegasus-large` - PEGASUS architecture, trained on news and scientific papers, designed for longer documents (thousands of words)\n",
        "   \n",
        "   **Why summarization is harder than classification:**  \n",
        "   - Requires understanding the entire document, not just local patterns\n",
        "   - Must identify what's important vs. what's detail\n",
        "   - Needs to maintain coherence and fluency in generated text\n",
        "   - Abstractive summarization requires generation capabilities (more complex than classification)\n",
        "   - Must handle variable-length outputs\n",
        "   - Evaluation is more subjective (multiple valid summaries possible)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 6: Machine Translation\n",
        "\n",
        "1. **Architecture of `Helsinki-NLP/opus-mt-en-de`:**  \n",
        "   - **Architecture:** MarianMT (Marian Neural Machine Translation) - an encoder-decoder transformer architecture optimized for translation\n",
        "   - **OPUS:** Open Parallel Corpus - a collection of translated texts from various sources, used for training translation models\n",
        "   - **MT:** Machine Translation\n",
        "\n",
        "2. **English→French translation models:**  \n",
        "   - `Helsinki-NLP/opus-mt-en-fr` - MarianMT model for English to French\n",
        "   - `facebook/mbart-large-50-many-to-many-mmt` - Multilingual BART model supporting English↔French\n",
        "   - `t5-base` or `t5-large` can be fine-tuned for translation tasks\n",
        "   - `Helsinki-NLP/opus-mt-en-roa` (Romance languages including French)\n",
        "\n",
        "3. **Bilingual vs Multilingual models:**  \n",
        "   - **Bilingual:** Trained on one language pair (e.g., English↔German). Pros: Often better quality for that specific pair, smaller model size. Cons: Need separate model for each language pair.\n",
        "   - **Multilingual:** Trained on many language pairs simultaneously. Pros: One model handles many languages, can leverage cross-lingual transfer. Cons: Larger models, may have lower quality per language pair, requires more training data.\n",
        "\n",
        "4. **How `\"translation_en_to_de\"` relates to model:**  \n",
        "   The task name `\"translation_en_to_de\"` tells the pipeline to load a model that translates from English (en) to German (de). When you also specify `model=\"Helsinki-NLP/opus-mt-en-de\"`, it uses that specific model. The task name helps the pipeline understand the translation direction and apply appropriate preprocessing.\n",
        "\n",
        "5. **What is `sacremoses`?**  \n",
        "   `sacremoses` is a Python library that provides sentence segmentation and tokenization tools, particularly for machine translation. It's used by MarianMT tokenizers for:\n",
        "   - Sentence splitting (breaking text into sentences)\n",
        "   - Tokenization (especially for languages with complex tokenization rules)\n",
        "   - Detokenization (converting tokens back to text)\n",
        "   The warning appears because `sacremoses` improves tokenization quality but isn't strictly required (the model works without it, just with potentially lower quality).\n",
        "\n",
        "6. **Multilingual translation models:**  \n",
        "   - **mBART (multilingual BART):** `facebook/mbart-large-50` supports 50 languages and can translate between many language pairs\n",
        "   - **M2M-100:** `facebook/m2m100_418M` or `facebook/m2m100_1.2B` supports 100 languages and can translate between any pair of those 100 languages (9,900 possible language pairs!)\n",
        "   - **OPUS-MT multilingual:** Various models covering multiple language families\n",
        "   \n",
        "   **Low-resource language challenges:**  \n",
        "   - Limited training data available\n",
        "   - Fewer parallel corpora\n",
        "   - May need to use transfer learning from high-resource languages\n",
        "   - Code-switching and dialect variations\n",
        "   - Evaluation metrics may not work well\n",
        "   - Need for specialized tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "summarizer = pipeline(\"summarization\")\n",
        "outputs = summarizer(text, max_length=45, min_length=56, clean_up_tokenization_spaces=True)\n",
        "print(outputs[0][\"summary_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 7: Text Generation\n",
        "\n",
        "1. **Default model and architecture:**  \n",
        "   - **Default model:** `gpt2` (GPT-2 base)\n",
        "   - **Architecture:** **Decoder-only transformer** (not encoder-decoder, not encoder-only). GPT-2 uses a stack of transformer decoder blocks with masked self-attention.\n",
        "   - **Parameters:** GPT-2 base has **124 million parameters**. Other sizes: GPT-2 small (117M), GPT-2 medium (345M), GPT-2 large (762M), GPT-2 XL (1.5B).\n",
        "   - **Generation type:** **Autoregressive generation** - generates tokens one at a time, using previously generated tokens as context for the next token.\n",
        "\n",
        "2. **Why `set_seed(42)`?**  \n",
        "   `set_seed(42)` sets the random seed for reproducibility. Without it:\n",
        "   - Each run produces different outputs (due to random sampling)\n",
        "   - Results are not reproducible\n",
        "   - Makes it difficult to compare results or debug\n",
        "   - With the seed, you get the same output every time (deterministic behavior)\n",
        "\n",
        "3. **Other generation parameters:**  \n",
        "   - **`temperature`:** Controls randomness. Lower (0.1-0.5) = more deterministic/focused. Higher (0.7-1.5) = more creative/random. Default is 1.0.\n",
        "   - **`top_k`:** Limits sampling to the top K most likely tokens. Reduces chance of low-probability tokens. `top_k=50` means only consider the 50 most likely next tokens.\n",
        "   - **`do_sample`:** If `True`, uses sampling (random selection based on probabilities). If `False`, uses greedy decoding (always picks most likely token). `do_sample=True` with `temperature=1.0` gives diverse outputs.\n",
        "\n",
        "4. **Truncation warning:**  \n",
        "   The truncation warning means the input text was too long for the model's maximum context length (GPT-2 has 1024 token limit). The pipeline automatically truncates the input to fit. This happens because:\n",
        "   - The prompt exceeds the model's context window\n",
        "   - The model can only process a fixed maximum number of tokens\n",
        "   - Truncation ensures the model can process the input, but you lose information from the truncated part\n",
        "\n",
        "5. **Setting `pad_token_id` to `eos_token_id`:**  \n",
        "   GPT-2 doesn't have a padding token by default (it wasn't trained with one). When batching sequences of different lengths, you need a padding token. Setting `pad_token_id=generator.tokenizer.eos_token_id` uses the end-of-sequence token as padding. This is necessary because:\n",
        "   - Without a pad token, the tokenizer/model may raise errors during batching\n",
        "   - Using EOS as padding is a common workaround\n",
        "   - It tells the model where sequences end (though not ideal, it works)\n",
        "\n",
        "6. **Trade-offs between model size and generation quality:**  \n",
        "   - **Larger models (GPT-2 XL, GPT-3):** Better coherence, more knowledge, better at following instructions, but require more memory, slower inference, higher cost\n",
        "   - **Smaller models (GPT-2 base):** Faster, less memory, lower cost, but may produce less coherent text, less knowledge, more repetition\n",
        "   - **Sweet spot:** Depends on use case - for simple tasks, smaller models suffice; for complex generation, larger models are worth the cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "translator = pipeline(\"translation_en_to_de\", model=\"Helsinki-NLP/opus-mt-en-de\")\n",
        "outputs = translator(text, clean_up_tokenization_spaces=True)\n",
        "print(outputs[0][\"translation_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline, set_seed\n",
        "\n",
        "generator = pipeline(\"text-generation\")\n",
        "set_seed(42)\n",
        "\n",
        "outputs = generator(\n",
        "    text,\n",
        "    max_length=200,\n",
        "    do_sample=True,\n",
        "    pad_token_id=generator.tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "print(outputs[0][\"generated_text\"])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
