{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# What is Natural Language Processing?\n",
        "\n",
        "Natural Language Processing (NLP) is a branch of artificial intelligence that focuses on enabling machines to understand, interpret, and generate human language in a valuable way. NLP combines computational linguistics with statistical, machine learning, and deep learning models.\n",
        "\n",
        "## Importance of NLP\n",
        "\n",
        "NLP is crucial because it bridges the gap between human communication and machine understanding. It enables computers to:\n",
        "- Process and analyze large amounts of natural language data\n",
        "- Extract meaningful information from text\n",
        "- Understand context and sentiment\n",
        "- Generate human-like responses\n",
        "\n",
        "## Applications\n",
        "\n",
        "- **Sentiment Analysis**: Understanding emotions and opinions in text\n",
        "- **Machine Translation**: Translating text between languages\n",
        "- **Chatbots**: Conversational AI systems\n",
        "- **Text Summarization**: Creating concise summaries of long documents\n",
        "- **Spam Detection**: Identifying unwanted emails\n",
        "- **Speech Recognition**: Converting speech to text\n",
        "\n",
        "## Challenges\n",
        "\n",
        "- **Ambiguity**: Words and phrases can have multiple meanings\n",
        "- **Context Understanding**: Meaning depends on surrounding text\n",
        "- **Data Scarcity**: Limited labeled data for training\n",
        "- **Language Variations**: Slang, dialects, and evolving language\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A First Glimpse of NLP\n",
        "\n",
        "The NLP pipeline typically follows these steps:\n",
        "\n",
        "1. **Text**: Raw data in human language\n",
        "2. **Tokens**: Breaking text into words, subwords, or characters\n",
        "3. **Encoded Vector**: Converting tokens to numerical representation (machine-readable)\n",
        "4. **Model**: Applying Machine Learning or Deep Learning models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tokenization\n",
        "\n",
        "**Tokenization** is the process of breaking down text into smaller units called tokens. These tokens can be words, characters, or subwords, depending on the tokenization strategy used.\n",
        "\n",
        "## Types of Tokenization\n",
        "\n",
        "### 1. Word Tokenization\n",
        "Splitting text into individual words. This is the most common approach.\n",
        "\n",
        "### 2. Character Tokenization\n",
        "Splitting text into individual characters. Useful for languages with no word boundaries or for character-level models.\n",
        "\n",
        "### 3. Subword Tokenization\n",
        "Splitting text into meaningful subword units (e.g., \"un-\", \"-ing\", \"book\" → \"book\"). This helps handle out-of-vocabulary words and reduces vocabulary size.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text to Tokens Example\n",
        "\n",
        "Let's see how the text **\"Don't judge a book by its cover\"** is tokenized:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# Download required NLTK data\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "# Example text\n",
        "text = \"Don't judge a book by its cover\"\n",
        "\n",
        "# Word tokenization\n",
        "tokens = word_tokenize(text)\n",
        "print(\"Original text:\", text)\n",
        "print(\"Tokens:\", tokens)\n",
        "print(f\"\\nNumber of tokens: {len(tokens)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tokens to Encoded Vector\n",
        "\n",
        "To process text with machine learning models, we need to convert tokens into numerical representations. This process is called **encoding** or **vectorization**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# Example tokens (as a sentence for vectorization)\n",
        "text = \"Don't judge a book by its cover\"\n",
        "\n",
        "# Using CountVectorizer (Bag of Words)\n",
        "count_vectorizer = CountVectorizer()\n",
        "count_matrix = count_vectorizer.fit_transform([text])\n",
        "\n",
        "print(\"Tokens:\", word_tokenize(text))\n",
        "print(\"\\nVocabulary:\", count_vectorizer.get_feature_names_out())\n",
        "print(\"\\nEncoded vector (Count):\")\n",
        "print(count_matrix.toarray())\n",
        "\n",
        "# Using TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform([text])\n",
        "\n",
        "print(\"\\nEncoded vector (TF-IDF):\")\n",
        "print(tfidf_matrix.toarray())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Word Embeddings and Vector Representation\n",
        "\n",
        "**Word Embeddings** are dense vector representations of words in a continuous vector space. Unlike sparse representations (like one-hot encoding), embeddings capture semantic relationships between words.\n",
        "\n",
        "## Key Concepts\n",
        "\n",
        "- **Dense Vectors**: Each word is represented by a dense vector of real numbers\n",
        "- **Semantic Relationships**: Words with similar meanings are close in the vector space\n",
        "- **Context-Aware**: Modern embeddings capture context-dependent meanings\n",
        "\n",
        "## Types of Word Embeddings\n",
        "\n",
        "1. **Word2Vec**: Predicts words from context (CBOW) or context from words (Skip-gram)\n",
        "2. **GloVe**: Global Vectors for Word Representation using co-occurrence statistics\n",
        "3. **FastText**: Extends Word2Vec with subword information\n",
        "4. **Contextual Embeddings**: BERT, ELMo, GPT (capture context-dependent meanings)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Vector Representation Methods\n",
        "\n",
        "### 1. One-Hot Encoding\n",
        "Each word is represented as a binary vector where only one element is 1 and all others are 0.\n",
        "\n",
        "### 2. TF-IDF (Term Frequency-Inverse Document Frequency)\n",
        "Weights words based on their frequency in a document relative to their frequency across all documents.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Example: One-Hot Encoding\n",
        "texts = [\"Don't judge a book by its cover\", \"A good book is a good friend\"]\n",
        "tokens_list = [word_tokenize(text.lower()) for text in texts]\n",
        "\n",
        "# Get all unique words\n",
        "all_words = set()\n",
        "for tokens in tokens_list:\n",
        "    all_words.update(tokens)\n",
        "all_words = sorted(list(all_words))\n",
        "\n",
        "print(\"Vocabulary:\", all_words)\n",
        "print(f\"Vocabulary size: {len(all_words)}\")\n",
        "\n",
        "# Create one-hot encoding\n",
        "one_hot_vectors = []\n",
        "for tokens in tokens_list:\n",
        "    vector = [1 if word in tokens else 0 for word in all_words]\n",
        "    one_hot_vectors.append(vector)\n",
        "    print(f\"\\nText: {' '.join(tokens)}\")\n",
        "    print(f\"One-hot vector: {vector}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TF-IDF Vectorization with multiple documents\n",
        "documents = [\n",
        "    \"Don't judge a book by its cover\",\n",
        "    \"A good book is a good friend\",\n",
        "    \"The cover of the book is beautiful\"\n",
        "]\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
        "\n",
        "print(\"Documents:\")\n",
        "for i, doc in enumerate(documents):\n",
        "    print(f\"{i+1}. {doc}\")\n",
        "\n",
        "print(\"\\nVocabulary:\", tfidf_vectorizer.get_feature_names_out())\n",
        "print(\"\\nTF-IDF Matrix:\")\n",
        "print(tfidf_matrix.toarray())\n",
        "print(\"\\nShape:\", tfidf_matrix.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Character Embeddings\n",
        "\n",
        "Character embeddings represent individual characters as vectors. This is particularly useful for:\n",
        "- Handling out-of-vocabulary words\n",
        "- Morphologically rich languages\n",
        "- Character-level language models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Character-level tokenization\n",
        "text = \"Don't judge a book by its cover\"\n",
        "\n",
        "# Character tokenization\n",
        "char_tokens = list(text)\n",
        "print(\"Original text:\", text)\n",
        "print(\"Character tokens:\", char_tokens)\n",
        "print(f\"Number of characters: {len(char_tokens)}\")\n",
        "\n",
        "# Character-level one-hot encoding\n",
        "unique_chars = sorted(set(char_tokens))\n",
        "char_to_idx = {char: idx for idx, char in enumerate(unique_chars)}\n",
        "\n",
        "print(f\"\\nUnique characters: {unique_chars}\")\n",
        "print(f\"Character to index mapping: {char_to_idx}\")\n",
        "\n",
        "# Create character-level one-hot vectors\n",
        "char_vectors = []\n",
        "for char in char_tokens[:10]:  # Show first 10 characters\n",
        "    vector = [1 if i == char_to_idx[char] else 0 for i in range(len(unique_chars))]\n",
        "    char_vectors.append(vector)\n",
        "    print(f\"'{char}' -> {vector[:5]}...\")  # Show first 5 dimensions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Applications of NLP\n",
        "\n",
        "## 1. Sentiment Analysis\n",
        "Analyzing emotions and opinions expressed in text to determine whether the sentiment is positive, negative, or neutral.\n",
        "\n",
        "## 2. Machine Translation\n",
        "Automatically translating text from one language to another while preserving meaning.\n",
        "\n",
        "## 3. Chatbots\n",
        "Conversational AI systems that can understand user queries and provide relevant responses.\n",
        "\n",
        "## 4. Text Summarization\n",
        "Creating concise summaries of long documents while preserving key information.\n",
        "\n",
        "## 5. Spam Detection\n",
        "Identifying and filtering unwanted or malicious emails and messages.\n",
        "\n",
        "## 6. Speech Recognition\n",
        "Converting spoken language into text format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Simple Sentiment Analysis using TF-IDF\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Sample data\n",
        "positive_texts = [\n",
        "    \"I love this product!\",\n",
        "    \"This is amazing and wonderful\",\n",
        "    \"Great quality, highly recommend\",\n",
        "    \"Excellent service and fast delivery\",\n",
        "    \"Best purchase I've ever made\"\n",
        "]\n",
        "\n",
        "negative_texts = [\n",
        "    \"This is terrible and disappointing\",\n",
        "    \"Poor quality, waste of money\",\n",
        "    \"Very bad experience\",\n",
        "    \"Not worth the price\",\n",
        "    \"I hate this product\"\n",
        "]\n",
        "\n",
        "# Create dataset\n",
        "texts = positive_texts + negative_texts\n",
        "labels = ['positive'] * len(positive_texts) + ['negative'] * len(negative_texts)\n",
        "\n",
        "# Vectorize\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(texts)\n",
        "y = labels\n",
        "\n",
        "# Train a simple classifier\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "classifier = MultinomialNB()\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = classifier.predict(X_test)\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Test on new text\n",
        "new_text = \"This product is really good!\"\n",
        "new_vector = vectorizer.transform([new_text])\n",
        "prediction = classifier.predict(new_vector)[0]\n",
        "print(f\"\\nText: '{new_text}'\")\n",
        "print(f\"Predicted sentiment: {prediction}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Challenges in NLP\n",
        "\n",
        "## 1. Ambiguity\n",
        "Words and phrases can have multiple meanings depending on context. For example:\n",
        "- \"Bank\" can mean a financial institution or the side of a river\n",
        "- \"I saw her duck\" - did she duck or did I see her pet duck?\n",
        "\n",
        "## 2. Context Understanding\n",
        "Understanding meaning requires context from surrounding words, sentences, and even broader discourse.\n",
        "\n",
        "## 3. Data Scarcity\n",
        "Many NLP tasks require large amounts of labeled training data, which can be expensive and time-consuming to create.\n",
        "\n",
        "## 4. Language Variations\n",
        "- Slang and informal language\n",
        "- Regional dialects\n",
        "- Evolving language over time\n",
        "- Multiple languages and code-switching\n",
        "\n",
        "## 5. Sarcasm and Irony\n",
        "Detecting when words mean the opposite of their literal meaning.\n",
        "\n",
        "## 6. Named Entity Recognition\n",
        "Identifying and classifying entities like names, locations, organizations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrating ambiguity challenge\n",
        "ambiguous_sentences = [\n",
        "    \"I saw her duck\",  # Did she duck or did I see her pet duck?\n",
        "    \"The bank is closed\",  # Financial institution or river bank?\n",
        "    \"Time flies like an arrow\",  # Time moves fast or insects that like arrows?\n",
        "    \"The chicken is ready to eat\"  # Ready to be eaten or ready to eat something?\n",
        "]\n",
        "\n",
        "print(\"Examples of Ambiguous Sentences:\")\n",
        "for i, sentence in enumerate(ambiguous_sentences, 1):\n",
        "    print(f\"{i}. {sentence}\")\n",
        "\n",
        "# Tokenization doesn't resolve ambiguity\n",
        "print(\"\\nTokenization of ambiguous sentences:\")\n",
        "for sentence in ambiguous_sentences:\n",
        "    tokens = word_tokenize(sentence)\n",
        "    print(f\"'{sentence}' -> {tokens}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Future of NLP\n",
        "\n",
        "The field of NLP is rapidly evolving with several exciting developments:\n",
        "\n",
        "## Recent Advances\n",
        "\n",
        "1. **Large Language Models (LLMs)**: Models like GPT, BERT, and T5 have revolutionized NLP\n",
        "2. **Transformer Architecture**: Attention mechanisms enable better context understanding\n",
        "3. **Multimodal Models**: Combining text, images, and audio\n",
        "4. **Few-Shot Learning**: Models that can learn from few examples\n",
        "5. **Zero-Shot Learning**: Models that can perform tasks they weren't explicitly trained on\n",
        "\n",
        "## Emerging Trends\n",
        "\n",
        "- **Conversational AI**: More natural and context-aware chatbots\n",
        "- **Multilingual Models**: Single models handling multiple languages\n",
        "- **Domain-Specific Models**: Specialized models for healthcare, legal, finance, etc.\n",
        "- **Efficient Models**: Smaller, faster models for edge devices\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ethical Considerations\n",
        "\n",
        "As NLP technology becomes more powerful, it's important to consider:\n",
        "\n",
        "## Key Ethical Issues\n",
        "\n",
        "1. **Bias and Fairness**: Models can perpetuate or amplify societal biases\n",
        "2. **Privacy**: Handling sensitive personal information in text data\n",
        "3. **Misinformation**: Potential for generating false or misleading content\n",
        "4. **Transparency**: Understanding how models make decisions\n",
        "5. **Accessibility**: Ensuring NLP benefits are available to all\n",
        "6. **Job Displacement**: Impact on employment in language-related fields\n",
        "\n",
        "## Responsible AI Development\n",
        "\n",
        "- Regular bias audits\n",
        "- Diverse training data\n",
        "- Transparent model documentation\n",
        "- User privacy protection\n",
        "- Continuous monitoring and evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conclusion\n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "1. **Tokenization** is the first step in NLP, breaking text into smaller units\n",
        "2. **Vectorization** converts tokens into numerical representations for machine processing\n",
        "3. **Word Embeddings** capture semantic relationships between words\n",
        "4. **NLP Applications** span from sentiment analysis to machine translation\n",
        "5. **Challenges** include ambiguity, context understanding, and data scarcity\n",
        "6. **Future** holds promise with large language models and multimodal approaches\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "- Explore pre-trained embeddings (Word2Vec, GloVe, FastText)\n",
        "- Experiment with transformer models (BERT, GPT)\n",
        "- Build practical NLP applications\n",
        "- Study advanced topics like attention mechanisms and transfer learning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Complete Example: Full NLP Pipeline\n",
        "\n",
        "# Step 1: Text Input\n",
        "text = \"Natural Language Processing is fascinating and powerful!\"\n",
        "\n",
        "# Step 2: Tokenization\n",
        "tokens = word_tokenize(text.lower())\n",
        "print(\"Step 1 - Text:\", text)\n",
        "print(\"Step 2 - Tokens:\", tokens)\n",
        "\n",
        "# Step 3: Vectorization\n",
        "vectorizer = TfidfVectorizer()\n",
        "vector = vectorizer.fit_transform([text])\n",
        "print(\"\\nStep 3 - Encoded Vector (TF-IDF):\")\n",
        "print(f\"Shape: {vector.shape}\")\n",
        "print(f\"Vector (first 10 values): {vector.toarray()[0][:10]}\")\n",
        "\n",
        "# Step 4: Vocabulary\n",
        "print(\"\\nVocabulary:\", vectorizer.get_feature_names_out())\n",
        "print(f\"Vocabulary size: {len(vectorizer.get_feature_names_out())}\")\n",
        "\n",
        "print(\"\\n✓ Complete NLP pipeline executed successfully!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
