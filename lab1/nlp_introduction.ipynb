{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# What is Natural Language Processing?\n",
        "\n",
        "Natural Language Processing (NLP) is a branch of artificial intelligence that focuses on enabling machines to understand, interpret, and generate human language in a valuable way. NLP combines computational linguistics with statistical, machine learning, and deep learning models.\n",
        "\n",
        "## Importance of NLP\n",
        "\n",
        "NLP is crucial because it bridges the gap between human communication and machine understanding. It enables computers to:\n",
        "- Process and analyze large amounts of natural language data\n",
        "- Extract meaningful information from text\n",
        "- Understand context and sentiment\n",
        "- Generate human-like responses\n",
        "\n",
        "## Applications\n",
        "\n",
        "- **Sentiment Analysis**: Understanding emotions and opinions in text\n",
        "- **Machine Translation**: Translating text between languages\n",
        "- **Chatbots**: Conversational AI systems\n",
        "- **Text Summarization**: Creating concise summaries of long documents\n",
        "- **Spam Detection**: Identifying unwanted emails\n",
        "- **Speech Recognition**: Converting speech to text\n",
        "\n",
        "## Challenges\n",
        "\n",
        "- **Ambiguity**: Words and phrases can have multiple meanings\n",
        "- **Context Understanding**: Meaning depends on surrounding text\n",
        "- **Data Scarcity**: Limited labeled data for training\n",
        "- **Language Variations**: Slang, dialects, and evolving language\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A First Glimpse of NLP\n",
        "\n",
        "The NLP pipeline typically follows these steps:\n",
        "\n",
        "1. **Text**: Raw data in human language\n",
        "2. **Tokens**: Breaking text into words, subwords, or characters\n",
        "3. **Encoded Vector**: Converting tokens to numerical representation (machine-readable)\n",
        "4. **Model**: Applying Machine Learning or Deep Learning models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tokenization\n",
        "\n",
        "**Tokenization** is the process of breaking down text into smaller units called tokens. These tokens can be words, characters, or subwords, depending on the tokenization strategy used.\n",
        "\n",
        "## Types of Tokenization\n",
        "\n",
        "### 1. Word Tokenization\n",
        "Splitting text into individual words. This is the most common approach.\n",
        "\n",
        "### 2. Character Tokenization\n",
        "Splitting text into individual characters. Useful for languages with no word boundaries or for character-level models.\n",
        "\n",
        "### 3. Subword Tokenization\n",
        "Splitting text into meaningful subword units (e.g., \"un-\", \"-ing\", \"book\" â†’ \"book\"). This helps handle out-of-vocabulary words and reduces vocabulary size.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text to Tokens Example\n",
        "\n",
        "Let's see how the text **\"Don't judge a book by its cover\"** is tokenized:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtokenize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m word_tokenize, sent_tokenize\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Download required NLTK data\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/__init__.py:147\u001b[39m\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mjsontags\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    143\u001b[39m \u001b[38;5;66;03m###########################################################\u001b[39;00m\n\u001b[32m    144\u001b[39m \u001b[38;5;66;03m# PACKAGES\u001b[39;00m\n\u001b[32m    145\u001b[39m \u001b[38;5;66;03m###########################################################\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchunk\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclassify\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minference\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/chunk/__init__.py:155\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Natural Language Toolkit: Chunkers\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Copyright (C) 2001-2025 NLTK Project\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# For license information, see LICENSE.TXT\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[33;03mClasses and interfaces for identifying non-overlapping linguistic\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[33;03mgroups (such as base noun phrases) in unrestricted text.  This task is\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    152\u001b[39m \u001b[33;03m     pattern is valid.\u001b[39;00m\n\u001b[32m    153\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchunk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChunkParserI\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchunk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnamed_entity\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Maxent_NE_Chunker\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchunk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mregexp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RegexpChunkParser, RegexpParser\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/chunk/api.py:13\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Natural Language Toolkit: Chunk parsing API\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Copyright (C) 2001-2025 NLTK Project\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m \u001b[38;5;66;03m##  Chunk Parser Interface\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m##//////////////////////////////////////////////////////\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchunk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChunkScore\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minternals\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deprecated\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ParserI\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/chunk/util.py:12\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m accuracy \u001b[38;5;28;01mas\u001b[39;00m _accuracy\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtag\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmapping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m map_tag\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtag\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m str2tuple\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tree\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/tag/__init__.py:72\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtag\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TaggerI\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtag\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m str2tuple, tuple2str, untag\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtag\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msequential\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     73\u001b[39m     SequentialBackoffTagger,\n\u001b[32m     74\u001b[39m     ContextTagger,\n\u001b[32m     75\u001b[39m     DefaultTagger,\n\u001b[32m     76\u001b[39m     NgramTagger,\n\u001b[32m     77\u001b[39m     UnigramTagger,\n\u001b[32m     78\u001b[39m     BigramTagger,\n\u001b[32m     79\u001b[39m     TrigramTagger,\n\u001b[32m     80\u001b[39m     AffixTagger,\n\u001b[32m     81\u001b[39m     RegexpTagger,\n\u001b[32m     82\u001b[39m     ClassifierBasedTagger,\n\u001b[32m     83\u001b[39m     ClassifierBasedPOSTagger,\n\u001b[32m     84\u001b[39m )\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtag\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbrill\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BrillTagger\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtag\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbrill_trainer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BrillTaggerTrainer\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/tag/sequential.py:26\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m List, Optional, Tuple\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m jsontags\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclassify\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NaiveBayesClassifier\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprobability\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConditionalFreqDist\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtag\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FeaturesetTaggerI, TaggerI\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/classify/__init__.py:97\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclassify\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpositivenaivebayes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PositiveNaiveBayesClassifier\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclassify\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrte_classify\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RTEFeatureExtractor, rte_classifier, rte_features\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclassify\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mscikitlearn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SklearnClassifier\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclassify\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msenna\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Senna\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclassify\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtextcat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TextCat\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/classify/scikitlearn.py:38\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprobability\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DictionaryProbDist\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature_extraction\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DictVectorizer\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/__init__.py:73\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# `_distributor_init` allows distributors to run custom init code.\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# For instance, for the Windows wheel, this is used to pre-load the\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# vcomp shared library runtime for OpenMP embedded in the sklearn/.libs\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401 E402\u001b[39;00m\n\u001b[32m     70\u001b[39m     __check_build,\n\u001b[32m     71\u001b[39m     _distributor_init,\n\u001b[32m     72\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clone  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_show_versions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_versions  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[32m     76\u001b[39m _submodules = [\n\u001b[32m     77\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcalibration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     78\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcluster\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    114\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcompose\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    115\u001b[39m ]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_metadata_requests\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester, _routing_enabled\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_missing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_scalar_nan\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/__init__.py:9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m metadata_routing\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_bunch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Bunch\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_chunking\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gen_batches, gen_even_slices\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Make _safe_indexing importable from here for backward compat as this particular\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# helper is considered semi-private and typically very useful for third-party\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# libraries that want to comply with scikit-learn's estimator API. In particular,\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# _safe_indexing was included in our public API documentation despite the leading\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# `_` in its name.\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_indexing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     17\u001b[39m     _safe_indexing,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m     18\u001b[39m     resample,\n\u001b[32m     19\u001b[39m     shuffle,\n\u001b[32m     20\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/_chunking.py:11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Interval, validate_params\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchunk_generator\u001b[39m(gen, chunksize):\n\u001b[32m     15\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Chunk generator, ``gen`` into lists of length ``chunksize``. The last\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[33;03m    chunk may have a length less than ``chunksize``.\"\"\"\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:17\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m csr_matrix, issparse\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvalidation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _is_arraylike_not_scalar\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mInvalidParameterError\u001b[39;00m(\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[32m     21\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Custom exception to be raised when the parameter of a class/method/function\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[33;03m    does not have a valid type or value.\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/validation.py:21\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_config \u001b[38;5;28;01mas\u001b[39;00m _get_config\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataConversionWarning, NotFittedError, PositiveSpectrumWarning\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_array_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _asarray_with_order, _is_numpy_namespace, get_namespace\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdeprecation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _deprecate_force_all_finite\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfixes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ComplexWarning, _preserve_dia_indices_dtype\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/_array_api.py:20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexternals\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array_api_extra \u001b[38;5;28;01mas\u001b[39;00m xpx\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexternals\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marray_api_compat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m numpy \u001b[38;5;28;01mas\u001b[39;00m np_compat\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfixes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parse_version\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# TODO: complete __all__\u001b[39;00m\n\u001b[32m     23\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33mxpx\u001b[39m\u001b[33m\"\u001b[39m]  \u001b[38;5;66;03m# we import xpx here just to re-export it, need this to appease ruff\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/fixes.py:20\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m optimize\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m     22\u001b[39m     pd = \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/__init__.py:139\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreshape\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    122\u001b[39m     concat,\n\u001b[32m    123\u001b[39m     lreshape,\n\u001b[32m   (...)\u001b[39m\u001b[32m    135\u001b[39m     qcut,\n\u001b[32m    136\u001b[39m )\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m api, arrays, errors, io, plotting, tseries\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m testing\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_print_versions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_versions\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    143\u001b[39m     \u001b[38;5;66;03m# excel\u001b[39;00m\n\u001b[32m    144\u001b[39m     ExcelFile,\n\u001b[32m   (...)\u001b[39m\u001b[32m    172\u001b[39m     read_spss,\n\u001b[32m    173\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/testing.py:6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mPublic testing utility functions.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_testing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      7\u001b[39m     assert_extension_array_equal,\n\u001b[32m      8\u001b[39m     assert_frame_equal,\n\u001b[32m      9\u001b[39m     assert_index_equal,\n\u001b[32m     10\u001b[39m     assert_series_equal,\n\u001b[32m     11\u001b[39m )\n\u001b[32m     13\u001b[39m __all__ = [\n\u001b[32m     14\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33massert_extension_array_equal\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     15\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33massert_frame_equal\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     16\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33massert_series_equal\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     17\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33massert_index_equal\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     18\u001b[39m ]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/_testing/__init__.py:44\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_testing\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_io\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     35\u001b[39m     round_trip_localpath,\n\u001b[32m     36\u001b[39m     round_trip_pathlib,\n\u001b[32m     37\u001b[39m     round_trip_pickle,\n\u001b[32m     38\u001b[39m     write_to_compressed,\n\u001b[32m     39\u001b[39m )\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_testing\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_warnings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     41\u001b[39m     assert_produces_warning,\n\u001b[32m     42\u001b[39m     maybe_produces_warning,\n\u001b[32m     43\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_testing\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01masserters\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     45\u001b[39m     assert_almost_equal,\n\u001b[32m     46\u001b[39m     assert_attr_equal,\n\u001b[32m     47\u001b[39m     assert_categorical_equal,\n\u001b[32m     48\u001b[39m     assert_class_equal,\n\u001b[32m     49\u001b[39m     assert_contains_all,\n\u001b[32m     50\u001b[39m     assert_copy,\n\u001b[32m     51\u001b[39m     assert_datetime_array_equal,\n\u001b[32m     52\u001b[39m     assert_dict_equal,\n\u001b[32m     53\u001b[39m     assert_equal,\n\u001b[32m     54\u001b[39m     assert_extension_array_equal,\n\u001b[32m     55\u001b[39m     assert_frame_equal,\n\u001b[32m     56\u001b[39m     assert_index_equal,\n\u001b[32m     57\u001b[39m     assert_indexing_slices_equivalent,\n\u001b[32m     58\u001b[39m     assert_interval_array_equal,\n\u001b[32m     59\u001b[39m     assert_is_sorted,\n\u001b[32m     60\u001b[39m     assert_is_valid_plot_return_object,\n\u001b[32m     61\u001b[39m     assert_metadata_equivalent,\n\u001b[32m     62\u001b[39m     assert_numpy_array_equal,\n\u001b[32m     63\u001b[39m     assert_period_array_equal,\n\u001b[32m     64\u001b[39m     assert_series_equal,\n\u001b[32m     65\u001b[39m     assert_sp_array_equal,\n\u001b[32m     66\u001b[39m     assert_timedelta_array_equal,\n\u001b[32m     67\u001b[39m     raise_assert_detail,\n\u001b[32m     68\u001b[39m )\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_testing\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     70\u001b[39m     get_dtype,\n\u001b[32m     71\u001b[39m     get_obj,\n\u001b[32m     72\u001b[39m )\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_testing\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcontexts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     74\u001b[39m     assert_cow_warning,\n\u001b[32m     75\u001b[39m     decompress_file,\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     with_csv_dialect,\n\u001b[32m     81\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/_testing/asserters.py:16\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmissing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_matching_na\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparseIndex\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtesting\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_testing\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtslibs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnp_datetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compare_mismatched_resolutions\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     20\u001b[39m     is_bool,\n\u001b[32m     21\u001b[39m     is_float_dtype,\n\u001b[32m   (...)\u001b[39m\u001b[32m     25\u001b[39m     needs_i8_conversion,\n\u001b[32m     26\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:645\u001b[39m, in \u001b[36mparent\u001b[39m\u001b[34m(self)\u001b[39m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# Download required NLTK data\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "# Example text\n",
        "text = \"Don't judge a book by its cover\"\n",
        "\n",
        "# Word tokenization\n",
        "tokens = word_tokenize(text)\n",
        "print(\"Original text:\", text)\n",
        "print(\"Tokens:\", tokens)\n",
        "print(f\"\\nNumber of tokens: {len(tokens)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tokens to Encoded Vector\n",
        "\n",
        "To process text with machine learning models, we need to convert tokens into numerical representations. This process is called **encoding** or **vectorization**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# Example tokens (as a sentence for vectorization)\n",
        "text = \"Don't judge a book by its cover\"\n",
        "\n",
        "# Using CountVectorizer (Bag of Words)\n",
        "count_vectorizer = CountVectorizer()\n",
        "count_matrix = count_vectorizer.fit_transform([text])\n",
        "\n",
        "print(\"Tokens:\", word_tokenize(text))\n",
        "print(\"\\nVocabulary:\", count_vectorizer.get_feature_names_out())\n",
        "print(\"\\nEncoded vector (Count):\")\n",
        "print(count_matrix.toarray())\n",
        "\n",
        "# Using TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform([text])\n",
        "\n",
        "print(\"\\nEncoded vector (TF-IDF):\")\n",
        "print(tfidf_matrix.toarray())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Word Embeddings and Vector Representation\n",
        "\n",
        "**Word Embeddings** are dense vector representations of words in a continuous vector space. Unlike sparse representations (like one-hot encoding), embeddings capture semantic relationships between words.\n",
        "\n",
        "## Key Concepts\n",
        "\n",
        "- **Dense Vectors**: Each word is represented by a dense vector of real numbers\n",
        "- **Semantic Relationships**: Words with similar meanings are close in the vector space\n",
        "- **Context-Aware**: Modern embeddings capture context-dependent meanings\n",
        "\n",
        "## Types of Word Embeddings\n",
        "\n",
        "1. **Word2Vec**: Predicts words from context (CBOW) or context from words (Skip-gram)\n",
        "2. **GloVe**: Global Vectors for Word Representation using co-occurrence statistics\n",
        "3. **FastText**: Extends Word2Vec with subword information\n",
        "4. **Contextual Embeddings**: BERT, ELMo, GPT (capture context-dependent meanings)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Vector Representation Methods\n",
        "\n",
        "### 1. One-Hot Encoding\n",
        "Each word is represented as a binary vector where only one element is 1 and all others are 0.\n",
        "\n",
        "### 2. TF-IDF (Term Frequency-Inverse Document Frequency)\n",
        "Weights words based on their frequency in a document relative to their frequency across all documents.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Example: One-Hot Encoding\n",
        "texts = [\"Don't judge a book by its cover\", \"A good book is a good friend\"]\n",
        "tokens_list = [word_tokenize(text.lower()) for text in texts]\n",
        "\n",
        "# Get all unique words\n",
        "all_words = set()\n",
        "for tokens in tokens_list:\n",
        "    all_words.update(tokens)\n",
        "all_words = sorted(list(all_words))\n",
        "\n",
        "print(\"Vocabulary:\", all_words)\n",
        "print(f\"Vocabulary size: {len(all_words)}\")\n",
        "\n",
        "# Create one-hot encoding\n",
        "one_hot_vectors = []\n",
        "for tokens in tokens_list:\n",
        "    vector = [1 if word in tokens else 0 for word in all_words]\n",
        "    one_hot_vectors.append(vector)\n",
        "    print(f\"\\nText: {' '.join(tokens)}\")\n",
        "    print(f\"One-hot vector: {vector}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TF-IDF Vectorization with multiple documents\n",
        "documents = [\n",
        "    \"Don't judge a book by its cover\",\n",
        "    \"A good book is a good friend\",\n",
        "    \"The cover of the book is beautiful\"\n",
        "]\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
        "\n",
        "print(\"Documents:\")\n",
        "for i, doc in enumerate(documents):\n",
        "    print(f\"{i+1}. {doc}\")\n",
        "\n",
        "print(\"\\nVocabulary:\", tfidf_vectorizer.get_feature_names_out())\n",
        "print(\"\\nTF-IDF Matrix:\")\n",
        "print(tfidf_matrix.toarray())\n",
        "print(\"\\nShape:\", tfidf_matrix.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Character Embeddings\n",
        "\n",
        "Character embeddings represent individual characters as vectors. This is particularly useful for:\n",
        "- Handling out-of-vocabulary words\n",
        "- Morphologically rich languages\n",
        "- Character-level language models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Character-level tokenization\n",
        "text = \"Don't judge a book by its cover\"\n",
        "\n",
        "# Character tokenization\n",
        "char_tokens = list(text)\n",
        "print(\"Original text:\", text)\n",
        "print(\"Character tokens:\", char_tokens)\n",
        "print(f\"Number of characters: {len(char_tokens)}\")\n",
        "\n",
        "# Character-level one-hot encoding\n",
        "unique_chars = sorted(set(char_tokens))\n",
        "char_to_idx = {char: idx for idx, char in enumerate(unique_chars)}\n",
        "\n",
        "print(f\"\\nUnique characters: {unique_chars}\")\n",
        "print(f\"Character to index mapping: {char_to_idx}\")\n",
        "\n",
        "# Create character-level one-hot vectors\n",
        "char_vectors = []\n",
        "for char in char_tokens[:10]:  # Show first 10 characters\n",
        "    vector = [1 if i == char_to_idx[char] else 0 for i in range(len(unique_chars))]\n",
        "    char_vectors.append(vector)\n",
        "    print(f\"'{char}' -> {vector[:5]}...\")  # Show first 5 dimensions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Applications of NLP\n",
        "\n",
        "## 1. Sentiment Analysis\n",
        "Analyzing emotions and opinions expressed in text to determine whether the sentiment is positive, negative, or neutral.\n",
        "\n",
        "## 2. Machine Translation\n",
        "Automatically translating text from one language to another while preserving meaning.\n",
        "\n",
        "## 3. Chatbots\n",
        "Conversational AI systems that can understand user queries and provide relevant responses.\n",
        "\n",
        "## 4. Text Summarization\n",
        "Creating concise summaries of long documents while preserving key information.\n",
        "\n",
        "## 5. Spam Detection\n",
        "Identifying and filtering unwanted or malicious emails and messages.\n",
        "\n",
        "## 6. Speech Recognition\n",
        "Converting spoken language into text format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Simple Sentiment Analysis using TF-IDF\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Sample data\n",
        "positive_texts = [\n",
        "    \"I love this product!\",\n",
        "    \"This is amazing and wonderful\",\n",
        "    \"Great quality, highly recommend\",\n",
        "    \"Excellent service and fast delivery\",\n",
        "    \"Best purchase I've ever made\"\n",
        "]\n",
        "\n",
        "negative_texts = [\n",
        "    \"This is terrible and disappointing\",\n",
        "    \"Poor quality, waste of money\",\n",
        "    \"Very bad experience\",\n",
        "    \"Not worth the price\",\n",
        "    \"I hate this product\"\n",
        "]\n",
        "\n",
        "# Create dataset\n",
        "texts = positive_texts + negative_texts\n",
        "labels = ['positive'] * len(positive_texts) + ['negative'] * len(negative_texts)\n",
        "\n",
        "# Vectorize\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(texts)\n",
        "y = labels\n",
        "\n",
        "# Train a simple classifier\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "classifier = MultinomialNB()\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = classifier.predict(X_test)\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Test on new text\n",
        "new_text = \"This product is really good!\"\n",
        "new_vector = vectorizer.transform([new_text])\n",
        "prediction = classifier.predict(new_vector)[0]\n",
        "print(f\"\\nText: '{new_text}'\")\n",
        "print(f\"Predicted sentiment: {prediction}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Challenges in NLP\n",
        "\n",
        "## 1. Ambiguity\n",
        "Words and phrases can have multiple meanings depending on context. For example:\n",
        "- \"Bank\" can mean a financial institution or the side of a river\n",
        "- \"I saw her duck\" - did she duck or did I see her pet duck?\n",
        "\n",
        "## 2. Context Understanding\n",
        "Understanding meaning requires context from surrounding words, sentences, and even broader discourse.\n",
        "\n",
        "## 3. Data Scarcity\n",
        "Many NLP tasks require large amounts of labeled training data, which can be expensive and time-consuming to create.\n",
        "\n",
        "## 4. Language Variations\n",
        "- Slang and informal language\n",
        "- Regional dialects\n",
        "- Evolving language over time\n",
        "- Multiple languages and code-switching\n",
        "\n",
        "## 5. Sarcasm and Irony\n",
        "Detecting when words mean the opposite of their literal meaning.\n",
        "\n",
        "## 6. Named Entity Recognition\n",
        "Identifying and classifying entities like names, locations, organizations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrating ambiguity challenge\n",
        "ambiguous_sentences = [\n",
        "    \"I saw her duck\",  # Did she duck or did I see her pet duck?\n",
        "    \"The bank is closed\",  # Financial institution or river bank?\n",
        "    \"Time flies like an arrow\",  # Time moves fast or insects that like arrows?\n",
        "    \"The chicken is ready to eat\"  # Ready to be eaten or ready to eat something?\n",
        "]\n",
        "\n",
        "print(\"Examples of Ambiguous Sentences:\")\n",
        "for i, sentence in enumerate(ambiguous_sentences, 1):\n",
        "    print(f\"{i}. {sentence}\")\n",
        "\n",
        "# Tokenization doesn't resolve ambiguity\n",
        "print(\"\\nTokenization of ambiguous sentences:\")\n",
        "for sentence in ambiguous_sentences:\n",
        "    tokens = word_tokenize(sentence)\n",
        "    print(f\"'{sentence}' -> {tokens}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Future of NLP\n",
        "\n",
        "The field of NLP is rapidly evolving with several exciting developments:\n",
        "\n",
        "## Recent Advances\n",
        "\n",
        "1. **Large Language Models (LLMs)**: Models like GPT, BERT, and T5 have revolutionized NLP\n",
        "2. **Transformer Architecture**: Attention mechanisms enable better context understanding\n",
        "3. **Multimodal Models**: Combining text, images, and audio\n",
        "4. **Few-Shot Learning**: Models that can learn from few examples\n",
        "5. **Zero-Shot Learning**: Models that can perform tasks they weren't explicitly trained on\n",
        "\n",
        "## Emerging Trends\n",
        "\n",
        "- **Conversational AI**: More natural and context-aware chatbots\n",
        "- **Multilingual Models**: Single models handling multiple languages\n",
        "- **Domain-Specific Models**: Specialized models for healthcare, legal, finance, etc.\n",
        "- **Efficient Models**: Smaller, faster models for edge devices\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ethical Considerations\n",
        "\n",
        "As NLP technology becomes more powerful, it's important to consider:\n",
        "\n",
        "## Key Ethical Issues\n",
        "\n",
        "1. **Bias and Fairness**: Models can perpetuate or amplify societal biases\n",
        "2. **Privacy**: Handling sensitive personal information in text data\n",
        "3. **Misinformation**: Potential for generating false or misleading content\n",
        "4. **Transparency**: Understanding how models make decisions\n",
        "5. **Accessibility**: Ensuring NLP benefits are available to all\n",
        "6. **Job Displacement**: Impact on employment in language-related fields\n",
        "\n",
        "## Responsible AI Development\n",
        "\n",
        "- Regular bias audits\n",
        "- Diverse training data\n",
        "- Transparent model documentation\n",
        "- User privacy protection\n",
        "- Continuous monitoring and evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conclusion\n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "1. **Tokenization** is the first step in NLP, breaking text into smaller units\n",
        "2. **Vectorization** converts tokens into numerical representations for machine processing\n",
        "3. **Word Embeddings** capture semantic relationships between words\n",
        "4. **NLP Applications** span from sentiment analysis to machine translation\n",
        "5. **Challenges** include ambiguity, context understanding, and data scarcity\n",
        "6. **Future** holds promise with large language models and multimodal approaches\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "- Explore pre-trained embeddings (Word2Vec, GloVe, FastText)\n",
        "- Experiment with transformer models (BERT, GPT)\n",
        "- Build practical NLP applications\n",
        "- Study advanced topics like attention mechanisms and transfer learning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Complete Example: Full NLP Pipeline\n",
        "\n",
        "# Step 1: Text Input\n",
        "text = \"Natural Language Processing is fascinating and powerful!\"\n",
        "\n",
        "# Step 2: Tokenization\n",
        "tokens = word_tokenize(text.lower())\n",
        "print(\"Step 1 - Text:\", text)\n",
        "print(\"Step 2 - Tokens:\", tokens)\n",
        "\n",
        "# Step 3: Vectorization\n",
        "vectorizer = TfidfVectorizer()\n",
        "vector = vectorizer.fit_transform([text])\n",
        "print(\"\\nStep 3 - Encoded Vector (TF-IDF):\")\n",
        "print(f\"Shape: {vector.shape}\")\n",
        "print(f\"Vector (first 10 values): {vector.toarray()[0][:10]}\")\n",
        "\n",
        "# Step 4: Vocabulary\n",
        "print(\"\\nVocabulary:\", vectorizer.get_feature_names_out())\n",
        "print(f\"Vocabulary size: {len(vectorizer.get_feature_names_out())}\")\n",
        "\n",
        "print(\"\\nâœ“ Complete NLP pipeline executed successfully!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
