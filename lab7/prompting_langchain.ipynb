{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 7: Prompting with LangChain & Ollama\n",
        "\n",
        "This notebook covers advanced prompting techniques and building applications with LangChain, including integration with local LLMs via Ollama.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this lab, you will be able to:\n",
        "- Understand prompt engineering principles\n",
        "- Use LangChain for building LLM applications\n",
        "- Set up and use Ollama for local LLM inference\n",
        "- Create chains, agents, and memory systems\n",
        "- Build practical NLP applications with LangChain\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction to Prompt Engineering\n",
        "\n",
        "**Prompt Engineering** is the art and science of crafting inputs (prompts) to get desired outputs from LLMs. Effective prompts:\n",
        "- Are clear and specific\n",
        "- Provide context and examples\n",
        "- Use appropriate formatting\n",
        "- Guide the model's reasoning process\n",
        "\n",
        "### LangChain Overview\n",
        "\n",
        "**LangChain** is a framework for building applications with LLMs that provides:\n",
        "- **Chains**: Sequences of operations\n",
        "- **Agents**: Autonomous decision-making systems\n",
        "- **Memory**: Conversation history management\n",
        "- **Tools**: Integration with external systems\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Imports successful!\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "# Uncomment if needed:\n",
        "# !pip install langchain langchain-community langchain-core ollama\n",
        "\n",
        "import os\n",
        "from typing import List, Dict\n",
        "\n",
        "print(\"✓ Imports successful!\")\n",
        "\n",
        "# Note: For Ollama, you need to:\n",
        "# 1. Install Ollama: https://ollama.ai\n",
        "# 2. Pull a model: ollama pull llama2\n",
        "# 3. Ensure Ollama is running locally\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Basic Prompting\n",
        "\n",
        "Let's start with fundamental prompt engineering techniques.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Zero-shot Prompt:\n",
            "\n",
            "Classify the sentiment of the following text as positive, negative, or neutral.\n",
            "\n",
            "Text: \"I absolutely love this new product! It's amazing.\"\n",
            "Sentiment:\n",
            "\n",
            "\n",
            "======================================================================\n",
            "Few-shot Prompt:\n",
            "\n",
            "Classify the sentiment of the following texts as positive, negative, or neutral.\n",
            "\n",
            "Text: \"This movie was terrible.\"\n",
            "Sentiment: negative\n",
            "\n",
            "Text: \"I enjoyed the concert very much.\"\n",
            "Sentiment: positive\n",
            "\n",
            "Text: \"The weather is okay today.\"\n",
            "Sentiment: neutral\n",
            "\n",
            "Text: \"This product exceeded my expectations!\"\n",
            "Sentiment:\n",
            "\n",
            "\n",
            "======================================================================\n",
            "Chain-of-Thought Prompt:\n",
            "\n",
            "Solve this math problem step by step.\n",
            "\n",
            "Problem: A store has 15 apples. They sell 6 apples in the morning and 4 apples in the afternoon. How many apples are left?\n",
            "\n",
            "Let's think step by step:\n",
            "1. Start with: 15 apples\n",
            "2. Morning sales: 15 - 6 = 9 apples\n",
            "3. Afternoon sales: 9 - 4 = 5 apples\n",
            "4. Answer: 5 apples are left\n",
            "\n",
            "Now solve this problem:\n",
            "Problem: A library has 50 books. They lend out 12 books on Monday and 8 books on Tuesday. How many books are left?\n",
            "\n",
            "Let's think step by step:\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Example 1: Zero-shot prompting\n",
        "zero_shot_prompt = \"\"\"\n",
        "Classify the sentiment of the following text as positive, negative, or neutral.\n",
        "\n",
        "Text: \"I absolutely love this new product! It's amazing.\"\n",
        "Sentiment:\n",
        "\"\"\"\n",
        "\n",
        "print(\"Zero-shot Prompt:\")\n",
        "print(zero_shot_prompt)\n",
        "\n",
        "# Example 2: Few-shot prompting\n",
        "few_shot_prompt = \"\"\"\n",
        "Classify the sentiment of the following texts as positive, negative, or neutral.\n",
        "\n",
        "Text: \"This movie was terrible.\"\n",
        "Sentiment: negative\n",
        "\n",
        "Text: \"I enjoyed the concert very much.\"\n",
        "Sentiment: positive\n",
        "\n",
        "Text: \"The weather is okay today.\"\n",
        "Sentiment: neutral\n",
        "\n",
        "Text: \"This product exceeded my expectations!\"\n",
        "Sentiment:\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Few-shot Prompt:\")\n",
        "print(few_shot_prompt)\n",
        "\n",
        "# Example 3: Chain-of-thought prompting\n",
        "cot_prompt = \"\"\"\n",
        "Solve this math problem step by step.\n",
        "\n",
        "Problem: A store has 15 apples. They sell 6 apples in the morning and 4 apples in the afternoon. How many apples are left?\n",
        "\n",
        "Let's think step by step:\n",
        "1. Start with: 15 apples\n",
        "2. Morning sales: 15 - 6 = 9 apples\n",
        "3. Afternoon sales: 9 - 4 = 5 apples\n",
        "4. Answer: 5 apples are left\n",
        "\n",
        "Now solve this problem:\n",
        "Problem: A library has 50 books. They lend out 12 books on Monday and 8 books on Tuesday. How many books are left?\n",
        "\n",
        "Let's think step by step:\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Chain-of-Thought Prompt:\")\n",
        "print(cot_prompt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: LangChain Basics\n",
        "\n",
        "Setting up LangChain with different LLM providers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangChain not installed. Install with: pip install langchain langchain-community\n",
            "\n",
            "For this demo, we'll show the structure without actual execution\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    from langchain.llms import Ollama\n",
        "    from langchain.prompts import PromptTemplate\n",
        "    from langchain.chains import LLMChain\n",
        "    \n",
        "    # Initialize Ollama (requires Ollama to be running locally)\n",
        "    # First, check if Ollama is available\n",
        "    try:\n",
        "        llm = Ollama(model=\"llama2\")\n",
        "        print(\"✓ Ollama LLM initialized (llama2)\")\n",
        "        print(\"Note: Ensure Ollama is running: ollama serve\")\n",
        "    except Exception as e:\n",
        "        print(f\"Ollama not available: {e}\")\n",
        "        print(\"Using a mock LLM for demonstration\")\n",
        "        llm = None\n",
        "    \n",
        "    # Alternative: Use Hugging Face models\n",
        "    try:\n",
        "        from langchain.llms import HuggingFacePipeline\n",
        "        from transformers import pipeline\n",
        "        \n",
        "        # This is a fallback if Ollama isn't available\n",
        "        # model_name = \"gpt2\"  # Small model for demo\n",
        "        # pipe = pipeline(\"text-generation\", model=model_name, max_length=100)\n",
        "        # llm = HuggingFacePipeline(pipeline=pipe)\n",
        "        print(\"Hugging Face integration available\")\n",
        "    except:\n",
        "        print(\"Hugging Face integration not configured\")\n",
        "    \n",
        "    print(\"\\nLangChain components ready!\")\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"LangChain not installed. Install with: pip install langchain langchain-community\")\n",
        "    print(\"\\nFor this demo, we'll show the structure without actual execution\")\n",
        "    llm = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Prompt Templates\n",
        "\n",
        "LangChain's PromptTemplate makes it easy to create reusable prompts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangChain not available. Here's what the template would look like:\n",
            "\n",
            "    Template:\n",
            "    You are a helpful assistant that explains complex topics in simple terms.\n",
            "\n",
            "    Topic: {topic}\n",
            "    Audience: {audience}\n",
            "\n",
            "    Please explain the topic in a way that is appropriate for {audience}.\n",
            "    \n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    from langchain.prompts import PromptTemplate\n",
        "    \n",
        "    # Create a prompt template\n",
        "    template = \"\"\"\n",
        "    You are a helpful assistant that explains complex topics in simple terms.\n",
        "    \n",
        "    Topic: {topic}\n",
        "    Audience: {audience}\n",
        "    \n",
        "    Please explain the topic in a way that is appropriate for {audience}.\n",
        "    \"\"\"\n",
        "    \n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"topic\", \"audience\"],\n",
        "        template=template\n",
        "    )\n",
        "    \n",
        "    # Format the prompt\n",
        "    formatted_prompt = prompt.format(\n",
        "        topic=\"quantum computing\",\n",
        "        audience=\"a 10-year-old\"\n",
        "    )\n",
        "    \n",
        "    print(\"Prompt Template Example:\")\n",
        "    print(\"=\"*70)\n",
        "    print(formatted_prompt)\n",
        "    \n",
        "    # If LLM is available, use it\n",
        "    if llm:\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"LLM Response:\")\n",
        "        print(\"=\"*70)\n",
        "        response = llm(formatted_prompt)\n",
        "        print(response)\n",
        "    else:\n",
        "        print(\"\\n(LLM not available - install Ollama or configure Hugging Face)\")\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"LangChain not available. Here's what the template would look like:\")\n",
        "    print(\"\"\"\n",
        "    Template:\n",
        "    You are a helpful assistant that explains complex topics in simple terms.\n",
        "    \n",
        "    Topic: {topic}\n",
        "    Audience: {audience}\n",
        "    \n",
        "    Please explain the topic in a way that is appropriate for {audience}.\n",
        "    \"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Chains\n",
        "\n",
        "Chains allow you to combine multiple LLM calls and other operations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangChain not available\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    from langchain.chains import LLMChain, SimpleSequentialChain\n",
        "    \n",
        "    if llm:\n",
        "        # Chain 1: Generate a story idea\n",
        "        story_template = \"\"\"\n",
        "        Generate a creative story idea about: {topic}\n",
        "        \n",
        "        Story idea:\n",
        "        \"\"\"\n",
        "        story_prompt = PromptTemplate(\n",
        "            input_variables=[\"topic\"],\n",
        "            template=story_template\n",
        "        )\n",
        "        story_chain = LLMChain(llm=llm, prompt=story_prompt)\n",
        "        \n",
        "        # Chain 2: Expand the story\n",
        "        expand_template = \"\"\"\n",
        "        Expand this story idea into a short paragraph:\n",
        "        \n",
        "        Story idea: {story_idea}\n",
        "        \n",
        "        Expanded story:\n",
        "        \"\"\"\n",
        "        expand_prompt = PromptTemplate(\n",
        "            input_variables=[\"story_idea\"],\n",
        "            template=expand_template\n",
        "        )\n",
        "        expand_chain = LLMChain(llm=llm, prompt=expand_prompt)\n",
        "        \n",
        "        # Combine chains\n",
        "        overall_chain = SimpleSequentialChain(\n",
        "            chains=[story_chain, expand_chain],\n",
        "            verbose=True\n",
        "        )\n",
        "        \n",
        "        print(\"Running sequential chain...\")\n",
        "        result = overall_chain.run(\"a robot learning to paint\")\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"Final Result:\")\n",
        "        print(\"=\"*70)\n",
        "        print(result)\n",
        "    else:\n",
        "        print(\"LLM not available. Here's how chains work:\")\n",
        "        print(\"\"\"\n",
        "        Chain Structure:\n",
        "        1. Story Chain: Generate story idea\n",
        "        2. Expand Chain: Expand the idea\n",
        "        \n",
        "        Sequential execution:\n",
        "        Input → Story Chain → Expand Chain → Output\n",
        "        \"\"\")\n",
        "        \n",
        "except ImportError:\n",
        "    print(\"LangChain not available\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"This is a demonstration of chain structure\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: Memory and Conversation\n",
        "\n",
        "LangChain provides memory systems for maintaining conversation context.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangChain not available\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    from langchain.memory import ConversationBufferMemory\n",
        "    from langchain.chains import ConversationChain\n",
        "    \n",
        "    if llm:\n",
        "        # Create memory\n",
        "        memory = ConversationBufferMemory()\n",
        "        \n",
        "        # Create conversation chain\n",
        "        conversation = ConversationChain(\n",
        "            llm=llm,\n",
        "            memory=memory,\n",
        "            verbose=True\n",
        "        )\n",
        "        \n",
        "        print(\"Conversation with Memory:\")\n",
        "        print(\"=\"*70)\n",
        "        \n",
        "        # First message\n",
        "        response1 = conversation.predict(input=\"Hi, my name is Alice. I'm learning about NLP.\")\n",
        "        print(f\"User: Hi, my name is Alice. I'm learning about NLP.\")\n",
        "        print(f\"Assistant: {response1}\\n\")\n",
        "        \n",
        "        # Second message (should remember the name)\n",
        "        response2 = conversation.predict(input=\"What did I just tell you about myself?\")\n",
        "        print(f\"User: What did I just tell you about myself?\")\n",
        "        print(f\"Assistant: {response2}\\n\")\n",
        "        \n",
        "        # Show memory\n",
        "        print(\"=\"*70)\n",
        "        print(\"Current Memory:\")\n",
        "        print(memory.buffer)\n",
        "    else:\n",
        "        print(\"LLM not available. Here's how memory works:\")\n",
        "        print(\"\"\"\n",
        "        Memory Types:\n",
        "        1. ConversationBufferMemory: Stores all messages\n",
        "        2. ConversationBufferWindowMemory: Stores last N messages\n",
        "        3. ConversationSummaryMemory: Summarizes conversation\n",
        "        \n",
        "        Example:\n",
        "        User: \"My name is Alice\"\n",
        "        Assistant: \"Nice to meet you, Alice!\"\n",
        "        [Memory stores: User name = Alice]\n",
        "        \n",
        "        User: \"What's my name?\"\n",
        "        Assistant: \"Your name is Alice\" [Uses memory]\n",
        "        \"\"\")\n",
        "        \n",
        "except ImportError:\n",
        "    print(\"LangChain not available\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"This demonstrates memory functionality\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 6: Agents\n",
        "\n",
        "Agents can use tools and make decisions autonomously.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangChain agents not available\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    from langchain.agents import initialize_agent, Tool\n",
        "    from langchain.agents import AgentType\n",
        "    \n",
        "    # Define custom tools\n",
        "    def calculate(expression: str) -> str:\n",
        "        \"\"\"Evaluate a mathematical expression\"\"\"\n",
        "        try:\n",
        "            result = eval(expression)\n",
        "            return str(result)\n",
        "        except:\n",
        "            return \"Error: Invalid expression\"\n",
        "    \n",
        "    def get_word_length(word: str) -> str:\n",
        "        \"\"\"Get the length of a word\"\"\"\n",
        "        return str(len(word))\n",
        "    \n",
        "    # Create tools\n",
        "    tools = [\n",
        "        Tool(\n",
        "            name=\"Calculator\",\n",
        "            func=calculate,\n",
        "            description=\"Useful for mathematical calculations. Input should be a valid Python expression.\"\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"WordLength\",\n",
        "            func=get_word_length,\n",
        "            description=\"Useful for getting the length of a word. Input should be a single word.\"\n",
        "        )\n",
        "    ]\n",
        "    \n",
        "    if llm:\n",
        "        # Initialize agent\n",
        "        agent = initialize_agent(\n",
        "            tools,\n",
        "            llm,\n",
        "            agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "            verbose=True\n",
        "        )\n",
        "        \n",
        "        print(\"Agent with Tools:\")\n",
        "        print(\"=\"*70)\n",
        "        \n",
        "        # Test the agent\n",
        "        result = agent.run(\"What is 15 multiplied by 8?\")\n",
        "        print(f\"\\nResult: {result}\")\n",
        "        \n",
        "        result2 = agent.run(\"What is the length of the word 'LangChain'?\")\n",
        "        print(f\"\\nResult: {result2}\")\n",
        "    else:\n",
        "        print(\"LLM not available. Here's how agents work:\")\n",
        "        print(\"\"\"\n",
        "        Agent Structure:\n",
        "        1. Tools: Functions the agent can use\n",
        "        2. LLM: Makes decisions about which tool to use\n",
        "        3. Agent: Orchestrates tool selection and execution\n",
        "        \n",
        "        Example Flow:\n",
        "        User: \"What is 10 * 5?\"\n",
        "        Agent thinks: \"I need to calculate this. I'll use the Calculator tool.\"\n",
        "        Agent uses Calculator tool with input \"10 * 5\"\n",
        "        Tool returns: \"50\"\n",
        "        Agent responds: \"The answer is 50\"\n",
        "        \"\"\")\n",
        "        \n",
        "except ImportError:\n",
        "    print(\"LangChain agents not available\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"This demonstrates agent functionality\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 7: Practical Application: Document Q&A\n",
        "\n",
        "Build a document question-answering system using LangChain.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Required packages not available:\n",
            "  - langchain.document_loaders\n",
            "  - langchain.vectorstores\n",
            "  - faiss-cpu (for vector storage)\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    from langchain.document_loaders import TextLoader\n",
        "    from langchain.text_splitter import CharacterTextSplitter\n",
        "    from langchain.embeddings import HuggingFaceEmbeddings\n",
        "    from langchain.vectorstores import FAISS\n",
        "    from langchain.chains import RetrievalQA\n",
        "    \n",
        "    # Sample document\n",
        "    sample_doc = \"\"\"\n",
        "    Natural Language Processing (NLP) is a branch of artificial intelligence that helps computers understand, \n",
        "    interpret and manipulate human language. NLP draws from many disciplines, including computer science and \n",
        "    computational linguistics, in its pursuit to fill the gap between human communication and computer understanding.\n",
        "    \n",
        "    Key NLP tasks include:\n",
        "    1. Tokenization: Breaking text into words or subwords\n",
        "    2. Named Entity Recognition: Identifying entities like names, locations\n",
        "    3. Sentiment Analysis: Determining emotional tone\n",
        "    4. Machine Translation: Translating between languages\n",
        "    5. Question Answering: Answering questions from text\n",
        "    \n",
        "    Modern NLP uses transformer models like BERT, GPT, and T5 which have revolutionized the field.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Save to temporary file\n",
        "    with open(\"temp_doc.txt\", \"w\") as f:\n",
        "        f.write(sample_doc)\n",
        "    \n",
        "    if llm:\n",
        "        # Load document\n",
        "        loader = TextLoader(\"temp_doc.txt\")\n",
        "        documents = loader.load()\n",
        "        \n",
        "        # Split into chunks\n",
        "        text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=50)\n",
        "        texts = text_splitter.split_documents(documents)\n",
        "        \n",
        "        # Create embeddings and vector store\n",
        "        embeddings = HuggingFaceEmbeddings()\n",
        "        vectorstore = FAISS.from_documents(texts, embeddings)\n",
        "        \n",
        "        # Create QA chain\n",
        "        qa_chain = RetrievalQA.from_chain_type(\n",
        "            llm=llm,\n",
        "            chain_type=\"stuff\",\n",
        "            retriever=vectorstore.as_retriever()\n",
        "        )\n",
        "        \n",
        "        # Ask questions\n",
        "        questions = [\n",
        "            \"What is NLP?\",\n",
        "            \"What are some key NLP tasks?\",\n",
        "            \"What models are used in modern NLP?\"\n",
        "        ]\n",
        "        \n",
        "        print(\"Document Q&A System:\")\n",
        "        print(\"=\"*70)\n",
        "        for question in questions:\n",
        "            answer = qa_chain.run(question)\n",
        "            print(f\"\\nQ: {question}\")\n",
        "            print(f\"A: {answer}\")\n",
        "    else:\n",
        "        print(\"LLM not available. Here's how document Q&A works:\")\n",
        "        print(\"\"\"\n",
        "        Document Q&A Pipeline:\n",
        "        1. Load documents\n",
        "        2. Split into chunks\n",
        "        3. Create embeddings\n",
        "        4. Store in vector database\n",
        "        5. Retrieve relevant chunks for question\n",
        "        6. Use LLM to generate answer from chunks\n",
        "        \n",
        "        This allows answering questions from large documents efficiently.\n",
        "        \"\"\")\n",
        "    \n",
        "    # Clean up\n",
        "    import os\n",
        "    if os.path.exists(\"temp_doc.txt\"):\n",
        "        os.remove(\"temp_doc.txt\")\n",
        "        \n",
        "except ImportError:\n",
        "    print(\"Required packages not available:\")\n",
        "    print(\"  - langchain.document_loaders\")\n",
        "    print(\"  - langchain.vectorstores\")\n",
        "    print(\"  - faiss-cpu (for vector storage)\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"This demonstrates document Q&A structure\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This lab covered:\n",
        "\n",
        "1. **Prompt Engineering**: Zero-shot, few-shot, and chain-of-thought prompting\n",
        "2. **LangChain Basics**: Setting up LLMs (Ollama, Hugging Face)\n",
        "3. **Prompt Templates**: Creating reusable, parameterized prompts\n",
        "4. **Chains**: Combining multiple LLM operations\n",
        "5. **Memory**: Maintaining conversation context\n",
        "6. **Agents**: Autonomous systems with tool usage\n",
        "7. **Document Q&A**: Building retrieval-augmented systems\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "- **Prompts matter**: Well-crafted prompts dramatically improve results\n",
        "- **LangChain simplifies**: Complex workflows become manageable\n",
        "- **Local LLMs**: Ollama enables running models locally\n",
        "- **Modularity**: Chains, agents, and memory are composable\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Experiment with different prompt templates\n",
        "- Build custom tools for agents\n",
        "- Explore advanced memory types (summary, token-based)\n",
        "- Integrate with external APIs and databases\n",
        "- Deploy LangChain applications\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
