{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 5: Clustering and Topic Modeling\n",
        "\n",
        "This notebook covers text clustering and topic modeling techniques, combining approaches from classical NLP and modern transformer-based embeddings.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this lab, you will be able to:\n",
        "- Generate dense embeddings using SentenceTransformers\n",
        "- Apply dimensionality reduction techniques (UMAP) for visualization\n",
        "- Perform clustering using HDBSCAN\n",
        "- Build topic models with BERTopic\n",
        "- Extract topic representations using KeyBERT and MMR\n",
        "- Integrate LLM-based topic labeling (optional)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction to Text Clustering\n",
        "\n",
        "Text clustering groups similar documents together without predefined categories. This is useful for:\n",
        "- **Topic Discovery**: Finding themes in large document collections\n",
        "- **Document Organization**: Automatically categorizing documents\n",
        "- **Data Exploration**: Understanding structure in unstructured text\n",
        "\n",
        "### Pipeline Overview\n",
        "\n",
        "1. **Embeddings**: Convert text to dense vector representations\n",
        "2. **Dimensionality Reduction**: Reduce dimensions for visualization (UMAP)\n",
        "3. **Clustering**: Group similar documents (HDBSCAN)\n",
        "4. **Topic Modeling**: Extract interpretable topics (BERTopic)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages if not already installed\n",
        "# Uncomment the following lines if needed:\n",
        "# !pip install sentence-transformers umap-learn hdbscan bertopic keybert\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"âœ“ All imports successful!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load and Prepare Data\n",
        "\n",
        "We'll use a subset of the 20 Newsgroups dataset for demonstration. In practice, you can use any collection of documents.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load a subset of 20 Newsgroups dataset\n",
        "# We'll use 4 categories for demonstration\n",
        "categories = ['sci.space', 'comp.graphics', 'rec.sport.baseball', 'talk.politics.mideast']\n",
        "\n",
        "print(\"Loading 20 Newsgroups dataset...\")\n",
        "newsgroups_train = fetch_20newsgroups(\n",
        "    subset='train',\n",
        "    categories=categories,\n",
        "    shuffle=True,\n",
        "    random_state=42,\n",
        "    remove=('headers', 'footers', 'quotes')\n",
        ")\n",
        "\n",
        "# Convert to DataFrame\n",
        "documents = newsgroups_train.data\n",
        "labels = newsgroups_train.target\n",
        "target_names = newsgroups_train.target_names\n",
        "\n",
        "# Filter out very short documents\n",
        "min_length = 50\n",
        "filtered_docs = []\n",
        "filtered_labels = []\n",
        "for doc, label in zip(documents, labels):\n",
        "    if len(doc) >= min_length:\n",
        "        filtered_docs.append(doc)\n",
        "        filtered_labels.append(label)\n",
        "\n",
        "documents = filtered_docs\n",
        "labels = np.array(filtered_labels)\n",
        "\n",
        "print(f\"âœ“ Loaded {len(documents)} documents\")\n",
        "print(f\"Categories: {target_names}\")\n",
        "print(f\"\\nSample document (first 200 chars):\")\n",
        "print(documents[0][:200] + \"...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Generate Embeddings with SentenceTransformers\n",
        "\n",
        "SentenceTransformers provide state-of-the-art sentence embeddings. We'll use a lightweight model (`all-MiniLM-L6-v2`) that balances performance and speed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    \n",
        "    print(\"Loading SentenceTransformer model...\")\n",
        "    # Using a lightweight model for faster processing\n",
        "    # For better quality, use 'all-mpnet-base-v2' or 'gte-small'\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    \n",
        "    print(\"Generating embeddings...\")\n",
        "    embeddings = model.encode(documents, show_progress_bar=True, batch_size=32)\n",
        "    \n",
        "    print(f\"âœ“ Generated embeddings: {embeddings.shape}\")\n",
        "    print(f\"  - Number of documents: {embeddings.shape[0]}\")\n",
        "    print(f\"  - Embedding dimension: {embeddings.shape[1]}\")\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"SentenceTransformers not installed.\")\n",
        "    print(\"Install with: pip install sentence-transformers\")\n",
        "    print(\"\\nUsing TF-IDF as fallback...\")\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    from sklearn.decomposition import TruncatedSVD\n",
        "    \n",
        "    vectorizer = TfidfVectorizer(max_features=100, stop_words='english')\n",
        "    tfidf = vectorizer.fit_transform(documents)\n",
        "    \n",
        "    # Reduce dimensions\n",
        "    svd = TruncatedSVD(n_components=50, random_state=42)\n",
        "    embeddings = svd.fit_transform(tfidf)\n",
        "    print(f\"âœ“ Generated TF-IDF + SVD embeddings: {embeddings.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Dimensionality Reduction with UMAP\n",
        "\n",
        "UMAP (Uniform Manifold Approximation and Projection) is excellent for visualizing high-dimensional embeddings while preserving local and global structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    import umap\n",
        "    \n",
        "    print(\"Applying UMAP dimensionality reduction...\")\n",
        "    umap_reducer = umap.UMAP(\n",
        "        n_components=2,\n",
        "        n_neighbors=15,\n",
        "        min_dist=0.1,\n",
        "        metric='cosine',\n",
        "        random_state=42\n",
        "    )\n",
        "    \n",
        "    embeddings_2d = umap_reducer.fit_transform(embeddings)\n",
        "    \n",
        "    print(f\"âœ“ Reduced to 2D: {embeddings_2d.shape}\")\n",
        "    \n",
        "    # Visualize\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    scatter = plt.scatter(\n",
        "        embeddings_2d[:, 0],\n",
        "        embeddings_2d[:, 1],\n",
        "        c=labels,\n",
        "        cmap='Spectral',\n",
        "        alpha=0.6,\n",
        "        s=50\n",
        "    )\n",
        "    plt.colorbar(scatter, label='Category')\n",
        "    plt.title('UMAP Visualization of Document Embeddings', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('UMAP Dimension 1')\n",
        "    plt.ylabel('UMAP Dimension 2')\n",
        "    \n",
        "    # Add legend\n",
        "    for i, name in enumerate(target_names):\n",
        "        plt.scatter([], [], label=name, alpha=0.6)\n",
        "    plt.legend(title='Categories', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"UMAP not installed. Install with: pip install umap-learn\")\n",
        "    print(\"Using PCA as fallback...\")\n",
        "    from sklearn.decomposition import PCA\n",
        "    \n",
        "    pca = PCA(n_components=2, random_state=42)\n",
        "    embeddings_2d = pca.fit_transform(embeddings)\n",
        "    \n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=labels, cmap='Spectral', alpha=0.6)\n",
        "    plt.title('PCA Visualization of Document Embeddings')\n",
        "    plt.xlabel('PC1')\n",
        "    plt.ylabel('PC2')\n",
        "    plt.colorbar(label='Category')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Clustering with HDBSCAN\n",
        "\n",
        "HDBSCAN (Hierarchical Density-Based Spatial Clustering) is ideal for text clustering because:\n",
        "- It doesn't require specifying the number of clusters\n",
        "- It identifies noise points (outliers)\n",
        "- It works well with density-based clustering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    import hdbscan\n",
        "    \n",
        "    print(\"Performing HDBSCAN clustering...\")\n",
        "    clusterer = hdbscan.HDBSCAN(\n",
        "        min_cluster_size=10,\n",
        "        min_samples=5,\n",
        "        metric='euclidean',\n",
        "        cluster_selection_method='eom'\n",
        "    )\n",
        "    \n",
        "    cluster_labels = clusterer.fit_predict(embeddings)\n",
        "    \n",
        "    n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
        "    n_noise = list(cluster_labels).count(-1)\n",
        "    \n",
        "    print(f\"âœ“ Clustering complete!\")\n",
        "    print(f\"  - Number of clusters: {n_clusters}\")\n",
        "    print(f\"  - Noise points: {n_noise}\")\n",
        "    print(f\"  - Cluster distribution: {np.bincount(cluster_labels[cluster_labels >= 0])}\")\n",
        "    \n",
        "    # Evaluate clustering quality (if we have ground truth)\n",
        "    if len(set(labels)) > 1:\n",
        "        ari = adjusted_rand_score(labels, cluster_labels)\n",
        "        nmi = normalized_mutual_info_score(labels, cluster_labels)\n",
        "        print(f\"\\nClustering Metrics (vs ground truth):\")\n",
        "        print(f\"  - Adjusted Rand Index: {ari:.3f}\")\n",
        "        print(f\"  - Normalized Mutual Info: {nmi:.3f}\")\n",
        "    \n",
        "    # Visualize clusters\n",
        "    plt.figure(figsize=(14, 6))\n",
        "    \n",
        "    plt.subplot(1, 2, 1)\n",
        "    scatter1 = plt.scatter(\n",
        "        embeddings_2d[:, 0],\n",
        "        embeddings_2d[:, 1],\n",
        "        c=cluster_labels,\n",
        "        cmap='Spectral',\n",
        "        alpha=0.6,\n",
        "        s=50\n",
        "    )\n",
        "    plt.colorbar(scatter1, label='Cluster')\n",
        "    plt.title('HDBSCAN Clustering Results', fontsize=12, fontweight='bold')\n",
        "    plt.xlabel('UMAP Dimension 1')\n",
        "    plt.ylabel('UMAP Dimension 2')\n",
        "    \n",
        "    plt.subplot(1, 2, 2)\n",
        "    scatter2 = plt.scatter(\n",
        "        embeddings_2d[:, 0],\n",
        "        embeddings_2d[:, 1],\n",
        "        c=labels,\n",
        "        cmap='Spectral',\n",
        "        alpha=0.6,\n",
        "        s=50\n",
        "    )\n",
        "    plt.colorbar(scatter2, label='True Category')\n",
        "    plt.title('Ground Truth Categories', fontsize=12, fontweight='bold')\n",
        "    plt.xlabel('UMAP Dimension 1')\n",
        "    plt.ylabel('UMAP Dimension 2')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"HDBSCAN not installed. Install with: pip install hdbscan\")\n",
        "    print(\"Using KMeans as fallback...\")\n",
        "    from sklearn.cluster import KMeans\n",
        "    \n",
        "    kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
        "    cluster_labels = kmeans.fit_predict(embeddings)\n",
        "    \n",
        "    print(f\"âœ“ KMeans clustering complete: {len(set(cluster_labels))} clusters\")\n",
        "    \n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=cluster_labels, cmap='Spectral', alpha=0.6)\n",
        "    plt.title('KMeans Clustering Results')\n",
        "    plt.colorbar(label='Cluster')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    from bertopic import BERTopic\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    \n",
        "    print(\"Initializing BERTopic model...\")\n",
        "    \n",
        "    # Use the same embedding model for consistency\n",
        "    embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    \n",
        "    # Initialize BERTopic\n",
        "    topic_model = BERTopic(\n",
        "        embedding_model=embedding_model,\n",
        "        top_n_words=10,\n",
        "        verbose=True\n",
        "    )\n",
        "    \n",
        "    print(\"Fitting BERTopic model...\")\n",
        "    topics, probs = topic_model.fit_transform(documents)\n",
        "    \n",
        "    print(f\"âœ“ BERTopic modeling complete!\")\n",
        "    print(f\"  - Number of topics discovered: {len(set(topics)) - (1 if -1 in topics else 0)}\")\n",
        "    \n",
        "    # Get topic information\n",
        "    topic_info = topic_model.get_topic_info()\n",
        "    print(f\"\\nTopic Information:\")\n",
        "    print(topic_info.head(10))\n",
        "    \n",
        "    # Visualize topics\n",
        "    try:\n",
        "        fig = topic_model.visualize_topics()\n",
        "        fig.show()\n",
        "    except:\n",
        "        print(\"Interactive visualization not available\")\n",
        "    \n",
        "    # Visualize barchart for top topics\n",
        "    try:\n",
        "        fig = topic_model.visualize_barchart(top_n_topics=5)\n",
        "        fig.show()\n",
        "    except:\n",
        "        print(\"Barchart visualization not available\")\n",
        "    \n",
        "    # Show sample documents for each topic\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"Sample Topics and Documents:\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    for topic_id in sorted(set(topics))[:5]:\n",
        "        if topic_id == -1:\n",
        "            print(f\"\\nðŸ“Œ Topic -1 (Outliers):\")\n",
        "        else:\n",
        "            words = topic_model.get_topic(topic_id)\n",
        "            top_words = [word for word, _ in words[:5]]\n",
        "            print(f\"\\nðŸ“Œ Topic {topic_id}: {', '.join(top_words)}\")\n",
        "        \n",
        "        # Show a sample document\n",
        "        topic_docs = [doc for doc, t in zip(documents, topics) if t == topic_id]\n",
        "        if topic_docs:\n",
        "            print(f\"   Sample: {topic_docs[0][:150]}...\")\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"BERTopic not installed. Install with: pip install bertopic\")\n",
        "    print(\"\\nUsing LDA as fallback...\")\n",
        "    from sklearn.feature_extraction.text import CountVectorizer\n",
        "    from sklearn.decomposition import LatentDirichletAllocation\n",
        "    \n",
        "    vectorizer = CountVectorizer(max_features=100, stop_words='english', min_df=2)\n",
        "    doc_term_matrix = vectorizer.fit_transform(documents)\n",
        "    \n",
        "    lda = LatentDirichletAllocation(n_components=4, random_state=42, max_iter=10)\n",
        "    lda.fit(doc_term_matrix)\n",
        "    \n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    print(\"\\nLDA Topics:\")\n",
        "    for topic_idx, topic in enumerate(lda.components_):\n",
        "        top_words_idx = topic.argsort()[-10:][::-1]\n",
        "        top_words = [feature_names[i] for i in top_words_idx]\n",
        "        print(f\"Topic {topic_idx}: {', '.join(top_words)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Topic Representation with KeyBERT\n",
        "\n",
        "KeyBERT extracts keywords that best represent a topic using BERT embeddings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    from keybert import KeyBERT\n",
        "    \n",
        "    print(\"Initializing KeyBERT...\")\n",
        "    kw_model = KeyBERT(model='all-MiniLM-L6-v2')\n",
        "    \n",
        "    # Extract keywords for each cluster\n",
        "    print(\"\\nExtracting keywords for each cluster...\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    for cluster_id in sorted(set(cluster_labels))[:5]:\n",
        "        if cluster_id == -1:\n",
        "            continue\n",
        "        \n",
        "        cluster_docs = [doc for doc, label in zip(documents, cluster_labels) if label == cluster_id]\n",
        "        cluster_text = ' '.join(cluster_docs[:5])  # Use first 5 docs as representative\n",
        "        \n",
        "        keywords = kw_model.extract_keywords(\n",
        "            cluster_text,\n",
        "            keyphrase_ngram_range=(1, 2),\n",
        "            stop_words='english',\n",
        "            top_n=5,\n",
        "            use_mmr=True,\n",
        "            diversity=0.5\n",
        "        )\n",
        "        \n",
        "        print(f\"\\nðŸ“Œ Cluster {cluster_id}:\")\n",
        "        for keyword, score in keywords:\n",
        "            print(f\"   - {keyword}: {score:.3f}\")\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"KeyBERT not installed. Install with: pip install keybert\")\n",
        "    print(\"\\nUsing TF-IDF keyword extraction as fallback...\")\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    \n",
        "    for cluster_id in sorted(set(cluster_labels))[:5]:\n",
        "        if cluster_id == -1:\n",
        "            continue\n",
        "        \n",
        "        cluster_docs = [doc for doc, label in zip(documents, cluster_labels) if label == cluster_id]\n",
        "        \n",
        "        vectorizer = TfidfVectorizer(max_features=10, stop_words='english')\n",
        "        tfidf = vectorizer.fit_transform(cluster_docs)\n",
        "        \n",
        "        feature_names = vectorizer.get_feature_names_out()\n",
        "        scores = tfidf.mean(axis=0).A1\n",
        "        top_indices = scores.argsort()[-5:][::-1]\n",
        "        \n",
        "        print(f\"\\nðŸ“Œ Cluster {cluster_id}:\")\n",
        "        for idx in top_indices:\n",
        "            print(f\"   - {feature_names[idx]}: {scores[idx]:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This lab covered:\n",
        "\n",
        "1. **Sentence Embeddings**: Using SentenceTransformers to create dense vector representations\n",
        "2. **Dimensionality Reduction**: UMAP for visualization of high-dimensional embeddings\n",
        "3. **Clustering**: HDBSCAN for density-based clustering without specifying cluster count\n",
        "4. **Topic Modeling**: BERTopic for extracting interpretable topics\n",
        "5. **Keyword Extraction**: KeyBERT for finding representative keywords\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "- **Embeddings matter**: Better embeddings lead to better clustering\n",
        "- **No free lunch**: Different clustering algorithms work better for different data\n",
        "- **Topic modeling is iterative**: Adjust parameters based on your data\n",
        "- **Visualization helps**: UMAP plots reveal cluster quality\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Experiment with different embedding models\n",
        "- Try different clustering algorithms (KMeans, DBSCAN, etc.)\n",
        "- Apply to your own document collections\n",
        "- Explore advanced BERTopic features (hierarchical topics, dynamic modeling)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
