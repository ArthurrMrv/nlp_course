{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello world Transformers ü§ó\n",
    "\n",
    "In this notebook we will explore the basics of the Hugging Face library by using a pre-trained model to classify text.\n",
    "\n",
    "‚ö†Ô∏è Do not forget to install the transformers library to run this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick overview of Transformer applications\n",
    "Let's start by defining a text that we will use to test the model.\n",
    "\n",
    "For testing purposes, we will use a text that is a complaint about a product. You can generate your own text or change the text to test the model with different inputs ü§ì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Dear Amazon, last week I ordered an Optimus Prime action figure \\\n",
    "from your online store in Germany. Unfortunately, when I opened the package, \\\n",
    "I discovered to my horror that I had been sent an action figure of Megatron \\\n",
    "instead! As a lifelong enemy of the Decepticons, I hope you can understand my \\\n",
    "dilemma. To resolve the issue, I demand an exchange of Megatron for the \\\n",
    "Optimus Prime figure I ordered. Enclosed are copies of my records concerning \\\n",
    "this purchase. I expect to hear from you soon. Sincerely, Bumblebee.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification\n",
    "\n",
    "## üìö Question 1: Understanding Pipelines\n",
    "Before we start using the models, let's understand what we're working with:\n",
    "\n",
    "1. What is a pipeline in Hugging Face Transformers? What does it abstract away from the user?\n",
    "2. Visit the pipeline documentation and list at least 3 other tasks (besides text-classification) that are available.\n",
    "3. What happens when you don't specify a model in the pipeline? How can you specify a specific model?\n",
    "\n",
    "üí° Hint: Check the official documentation to answer these questions!\n",
    "\n",
    "First thing we will do is to classify the text into two categories: positive or negative.\n",
    "\n",
    "To do this, we will use a pre-trained model from the Hugging Face library.\n",
    "\n",
    "We will use the pipeline function to load the model and the text-classification task.\n",
    "\n",
    "See the documentation for more details: https://huggingface.co/docs/transformers/main/en/pipeline_tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"text-classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Question 2: Text Classification Deep Dive\n",
    "Now that you've seen text classification in action, explore further:\n",
    "\n",
    "1. What is the default model used for text-classification? Look at the output above to find its name, then search for it on the Hugging Face Model Hub.\n",
    "2. What dataset was this model fine-tuned on? What kind of text does it work best with?\n",
    "3. The output includes a score field. What does this score represent? What range of values can it have?\n",
    "4. Challenge: Find a different text-classification model on the Hub that classifies emotions (not just positive/negative). What is its name?\n",
    "\n",
    "üí° Click on the model card in the Hub to see detailed information about training data and performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "outputs = classifier(text)\n",
    "pd.DataFrame(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "outputs = ner_tagger(text)\n",
    "pd.DataFrame(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Question 3: Named Entity Recognition (NER)\n",
    "Let's understand NER better:\n",
    "\n",
    "1. What does the `aggregation_strategy=\"simple\"` parameter do in the NER pipeline? Check the token classification documentation.\n",
    "2. Looking at the output above, what do the entity types mean? (ORG, MISC, LOC, PER)\n",
    "3. Why do some words appear with `##` prefix (like `##tron` and `##icons`)? What does this indicate about tokenization?\n",
    "4. The model seems to have split \"Megatron\" and \"Decepticons\" incorrectly. Why might this happen? What does this tell you about the model's training data?\n",
    "5. Challenge: Find the model card for `dbmdz/bert-large-cased-finetuned-conll03-english`. What is the CoNLL-2003 dataset?\n",
    "\n",
    "ü§î How might the choice of tokenizer affect NER performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = pipeline(\"question-answering\")\n",
    "question = \"What does the customer want?\"\n",
    "outputs = reader(question=question, context=text)\n",
    "pd.DataFrame([outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Question 4: Question Answering Systems\n",
    "Explore how question answering works:\n",
    "\n",
    "1. What type of question answering is this? (Extractive vs. Generative) Check the question answering documentation.\n",
    "2. The model outputs start and end indices. What do these represent? Why are they important?\n",
    "3. What is the SQuAD dataset? (Look up the model `distilbert-base-cased-distilled-squad` on the Hub)\n",
    "4. Try to think of a question this model CANNOT answer based on the text. Why would it fail?\n",
    "5. Challenge: What's the difference between extractive and generative question answering? Find an example of a generative QA model on the Hub.\n",
    "\n",
    "üí° Try asking questions that require reasoning or information not in the text. What happens?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarization\n",
    "\n",
    "## üìö Question 5: Text Summarization\n",
    "Before running the summarization code, let's understand how it works:\n",
    "\n",
    "1. What is the difference between extractive and abstractive summarization? Check the summarization documentation.\n",
    "\n",
    "Looking at the code in the next cell, what is the default model used for summarization? Search for it on the Hugging Face Model Hub and determine:\n",
    "\n",
    "- Is it an extractive or abstractive model?\n",
    "- What architecture does it use? (Hint: look at the model name)\n",
    "- What dataset was it trained on?\n",
    "- What do the `max_length` and `min_length` parameters control? What happens if `min_length > max_length`?\n",
    "\n",
    "The parameter `clean_up_tokenization_spaces=True` is used. What does this parameter do? Why might it be useful for summarization?\n",
    "\n",
    "Challenge: Find two different summarization models on the Hub:\n",
    "- One optimized for short texts (like news articles)\n",
    "- One that can handle longer documents\n",
    "\n",
    "Compare their architectures and training data.\n",
    "\n",
    "üí° Why might summarization be more challenging than text classification? What linguistic capabilities does the model need?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline(\"summarization\")\n",
    "outputs = summarizer(text, max_length=45, clean_up_tokenization_spaces=True)\n",
    "print(outputs[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation\n",
    "\n",
    "## üìö Question 6: Machine Translation\n",
    "Let's explore how translation models work:\n",
    "\n",
    "1. What is the architecture behind the `Helsinki-NLP/opus-mt-en-de` model? Look it up on the Model Hub.\n",
    "2. What does \"OPUS\" stand for?\n",
    "3. What does \"MT\" stand for?\n",
    "4. How would you find a model to translate from English to French? Visit the translation documentation and the Model Hub to find at least 2 different models.\n",
    "5. What is the difference between bilingual and multilingual translation models? What are the advantages and disadvantages of each?\n",
    "6. In the code, we specify the task as `\"translation_en_to_de\"`. How does this relate to the model we're loading?\n",
    "7. The output shows a warning about `sacremoses`. What is this library used for in NLP? Check the MarianMT documentation.\n",
    "8. Challenge: Find a multilingual model (like mBART or M2M100) that can translate between multiple language pairs. How many language pairs does it support?\n",
    "\n",
    "üåç What challenges exist for low-resource languages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = pipeline(\"translation_en_to_de\", \n",
    "                      model=\"Helsinki-NLP/opus-mt-en-de\")\n",
    "outputs = translator(text, clean_up_tokenization_spaces=True, min_length=100)\n",
    "print(outputs[0]['translation_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation\n",
    "\n",
    "## üìö Question 7: Text Generation\n",
    "Understand how language models generate text:\n",
    "\n",
    "1. What is the default model used for text generation in the code below? Look it up on the Hub and answer:\n",
    "   - What architecture does GPT-2 use? (decoder-only, encoder-decoder, or encoder-only?)\n",
    "   - How many parameters does the base GPT-2 model have?\n",
    "   - What type of generation does it perform? (autoregressive, non-autoregressive, etc.)\n",
    "2. Why do we use `set_seed(42)` before generation? What would happen without it? Check the generation documentation.\n",
    "3. The code uses `max_length=200`. What other parameters can control text generation? Research and explain:\n",
    "   - `temperature`\n",
    "   - `top_k`\n",
    "   - `do_sample`\n",
    "4. Looking at the output, you can see a warning about truncation. What does this mean? Why is the input being truncated?\n",
    "5. What does `pad_token_id` being set to `eos_token_id` mean? Why is this necessary for GPT-2?\n",
    "6. What are the trade-offs between model size and generation quality?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "set_seed(42) # Set the seed to get reproducible results\n",
    "generator = pipeline(\"text-generation\")\n",
    "response = \"Dear Bumblebee, I am sorry to hear that your order was mixed up.\"\n",
    "prompt = text + \"\\n\\nCustomer service response:\\n\" + response\n",
    "outputs = generator(prompt, max_length=200)\n",
    "print(outputs[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the model inside the pipeline to see other models. Try also other languages üåç"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
