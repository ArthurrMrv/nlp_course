{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 6: Fine-tuning Large Language Models\n",
        "\n",
        "This notebook covers fine-tuning large language models (LLMs) using parameter-efficient techniques like LoRA (Low-Rank Adaptation).\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this lab, you will be able to:\n",
        "- Load and prepare datasets for LLM fine-tuning\n",
        "- Understand LoRA and parameter-efficient fine-tuning\n",
        "- Fine-tune Llama models using Hugging Face\n",
        "- Evaluate fine-tuned models\n",
        "- Use fine-tuned models for inference\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction to LLM Fine-tuning\n",
        "\n",
        "Fine-tuning adapts pre-trained language models to specific tasks or domains. Instead of training from scratch, we:\n",
        "1. Start with a pre-trained model (e.g., Llama, GPT)\n",
        "2. Add task-specific layers or adapt existing parameters\n",
        "3. Train on domain-specific data\n",
        "\n",
        "### Why LoRA?\n",
        "\n",
        "**LoRA (Low-Rank Adaptation)** is a parameter-efficient fine-tuning method that:\n",
        "- Adds trainable low-rank matrices to existing weights\n",
        "- Reduces memory requirements significantly\n",
        "- Maintains model performance\n",
        "- Allows multiple task-specific adapters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "# Uncomment if needed:\n",
        "# !pip install transformers datasets accelerate peft bitsandbytes\n",
        "\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "import os\n",
        "\n",
        "print(\"âœ“ Imports successful!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load Dataset\n",
        "\n",
        "We'll use a medical Q&A dataset for demonstration. In practice, you can use any domain-specific dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For demonstration, we'll create a simple Q&A dataset\n",
        "# In practice, you would load from Hugging Face Hub or your own data\n",
        "\n",
        "# Example: Medical Q&A format\n",
        "sample_data = {\n",
        "    \"instruction\": [\n",
        "        \"What are the symptoms of diabetes?\",\n",
        "        \"How is hypertension treated?\",\n",
        "        \"What causes migraines?\",\n",
        "        \"Explain the function of the heart.\",\n",
        "        \"What is the difference between a virus and bacteria?\"\n",
        "    ],\n",
        "    \"input\": [\"\", \"\", \"\", \"\", \"\"],\n",
        "    \"output\": [\n",
        "        \"Common symptoms of diabetes include increased thirst, frequent urination, extreme fatigue, blurred vision, and slow-healing sores.\",\n",
        "        \"Hypertension is typically treated with lifestyle changes (diet, exercise) and medications such as ACE inhibitors, beta-blockers, or diuretics.\",\n",
        "        \"Migraines can be caused by various factors including stress, hormonal changes, certain foods, sleep patterns, and environmental triggers.\",\n",
        "        \"The heart pumps blood throughout the body, delivering oxygen and nutrients to tissues and removing carbon dioxide and waste products.\",\n",
        "        \"Viruses are smaller than bacteria, require a host cell to reproduce, and are not considered living organisms. Bacteria are single-celled organisms that can reproduce independently.\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Convert to dataset format\n",
        "from datasets import Dataset\n",
        "\n",
        "dataset = Dataset.from_dict(sample_data)\n",
        "\n",
        "# For a real scenario, you might load from Hugging Face:\n",
        "# dataset = load_dataset(\"medical_questions\", split=\"train\")\n",
        "\n",
        "print(f\"âœ“ Dataset loaded: {len(dataset)} examples\")\n",
        "print(\"\\nSample entry:\")\n",
        "print(dataset[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Load Pre-trained Model\n",
        "\n",
        "We'll use a smaller Llama model for demonstration. For production, consider larger models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model configuration\n",
        "model_name = \"microsoft/DialoGPT-small\"  # Using a smaller model for demo\n",
        "# For Llama models, you would use: \"meta-llama/Llama-2-7b-hf\" (requires access)\n",
        "\n",
        "print(f\"Loading model: {model_name}\")\n",
        "print(\"Note: For Llama models, you need Hugging Face access token\")\n",
        "\n",
        "try:\n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    \n",
        "    # Add padding token if it doesn't exist\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "    # Load model\n",
        "    # For 8-bit quantization (memory efficient):\n",
        "    # model = AutoModelForCausalLM.from_pretrained(\n",
        "    #     model_name,\n",
        "    #     load_in_8bit=True,\n",
        "    #     device_map=\"auto\"\n",
        "    # )\n",
        "    \n",
        "    # For regular loading:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "        device_map=\"auto\" if torch.cuda.is_available() else None\n",
        "    )\n",
        "    \n",
        "    print(f\"âœ“ Model loaded: {model_name}\")\n",
        "    print(f\"  - Model size: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M parameters\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "    print(\"\\nUsing a simpler approach for demonstration...\")\n",
        "    print(\"In practice, ensure you have:\")\n",
        "    print(\"1. Hugging Face account and access token\")\n",
        "    print(\"2. Sufficient GPU memory\")\n",
        "    print(\"3. Model access permissions (for Llama)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Prepare Data for Training\n",
        "\n",
        "Format the dataset into prompts that the model can learn from.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_prompt(instruction, input_text, output):\n",
        "    \"\"\"Format data into a prompt for instruction tuning\"\"\"\n",
        "    if input_text:\n",
        "        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n### Response:\\n{output}\"\n",
        "    else:\n",
        "        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n{output}\"\n",
        "    return prompt\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    \"\"\"Tokenize the formatted prompts\"\"\"\n",
        "    # Format prompts\n",
        "    texts = [\n",
        "        format_prompt(inst, inp, out)\n",
        "        for inst, inp, out in zip(\n",
        "            examples[\"instruction\"],\n",
        "            examples[\"input\"],\n",
        "            examples[\"output\"]\n",
        "        )\n",
        "    ]\n",
        "    \n",
        "    # Tokenize\n",
        "    tokenized = tokenizer(\n",
        "        texts,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    \n",
        "    # For causal LM, labels are the same as input_ids\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
        "    \n",
        "    return tokenized\n",
        "\n",
        "# Apply tokenization\n",
        "print(\"Tokenizing dataset...\")\n",
        "tokenized_dataset = dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=dataset.column_names\n",
        ")\n",
        "\n",
        "print(f\"âœ“ Dataset tokenized\")\n",
        "print(f\"  - Number of examples: {len(tokenized_dataset)}\")\n",
        "print(f\"\\nSample tokenized input (first 50 tokens):\")\n",
        "sample_ids = tokenized_dataset[0][\"input_ids\"][:50]\n",
        "print(tokenizer.decode(sample_ids))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Configure LoRA\n",
        "\n",
        "Set up LoRA for parameter-efficient fine-tuning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    from peft import LoraConfig, get_peft_model, TaskType\n",
        "    \n",
        "    # Configure LoRA\n",
        "    lora_config = LoraConfig(\n",
        "        r=16,  # Rank: lower = fewer parameters\n",
        "        lora_alpha=32,  # Scaling factor\n",
        "        target_modules=[\"q_lin\", \"k_lin\", \"v_lin\", \"out_lin\"],  # Attention layers\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=TaskType.CAUSAL_LM\n",
        "    )\n",
        "    \n",
        "    # Prepare model for LoRA\n",
        "    # If using 8-bit: model = prepare_model_for_kbit_training(model)\n",
        "    model = get_peft_model(model, lora_config)\n",
        "    \n",
        "    # Print trainable parameters\n",
        "    model.print_trainable_parameters()\n",
        "    \n",
        "    print(\"âœ“ LoRA configured successfully!\")\n",
        "    print(\"\\nLoRA adds trainable parameters to specific layers\")\n",
        "    print(\"This allows fine-tuning with much less memory\")\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"PEFT not installed. Install with: pip install peft\")\n",
        "    print(\"\\nFor this demo, we'll proceed without LoRA\")\n",
        "    print(\"In production, always use LoRA for efficient fine-tuning\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Training Setup\n",
        "\n",
        "Configure training arguments and start fine-tuning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./llm_finetuned\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    warmup_steps=10,\n",
        "    logging_steps=5,\n",
        "    save_steps=50,\n",
        "    evaluation_strategy=\"no\",\n",
        "    save_total_limit=2,\n",
        "    prediction_loss_only=True,\n",
        "    remove_unused_columns=False,\n",
        "    fp16=torch.cuda.is_available(),  # Use mixed precision if GPU available\n",
        "    report_to=\"none\"  # Disable wandb/tensorboard\n",
        ")\n",
        "\n",
        "# Data collator\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False  # Causal LM, not masked LM\n",
        ")\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"âœ“ Trainer configured\")\n",
        "print(f\"  - Training epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"  - Batch size: {training_args.per_device_train_batch_size}\")\n",
        "print(f\"  - Output directory: {training_args.output_dir}\")\n",
        "\n",
        "# Note: For this demo with very small dataset, training might overfit quickly\n",
        "# In practice, use larger datasets and proper train/val splits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Fine-tune the Model\n",
        "\n",
        "Train the model on your dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Starting training...\")\n",
        "print(\"Note: This is a demonstration. For real training:\")\n",
        "print(\"  - Use larger datasets\")\n",
        "print(\"  - Monitor validation loss\")\n",
        "print(\"  - Use proper train/val/test splits\")\n",
        "print(\"  - Adjust hyperparameters\")\n",
        "\n",
        "# Uncomment to actually train:\n",
        "# trainer.train()\n",
        "\n",
        "# Save the model\n",
        "# trainer.save_model()\n",
        "# tokenizer.save_pretrained(\"./llm_finetuned\")\n",
        "\n",
        "print(\"\\nâœ“ Training setup complete!\")\n",
        "print(\"Uncomment trainer.train() to start actual training\")\n",
        "print(\"\\nAfter training, you can:\")\n",
        "print(\"1. Load the fine-tuned model\")\n",
        "print(\"2. Test on new examples\")\n",
        "print(\"3. Compare with base model\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Inference with Fine-tuned Model\n",
        "\n",
        "Test the fine-tuned model on new examples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_response(model, tokenizer, instruction, max_length=200):\n",
        "    \"\"\"Generate response using the fine-tuned model\"\"\"\n",
        "    prompt = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n\"\n",
        "    \n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    if torch.cuda.is_available():\n",
        "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=1,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # Extract just the response part\n",
        "    if \"### Response:\" in response:\n",
        "        response = response.split(\"### Response:\")[-1].strip()\n",
        "    \n",
        "    return response\n",
        "\n",
        "# Test examples\n",
        "test_instructions = [\n",
        "    \"What are the symptoms of diabetes?\",\n",
        "    \"How is hypertension treated?\",\n",
        "    \"Explain the function of the heart.\"\n",
        "]\n",
        "\n",
        "print(\"Testing fine-tuned model (if trained):\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Note: This will use the base model if not fine-tuned\n",
        "# In practice, load your fine-tuned model:\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"./llm_finetuned\")\n",
        "\n",
        "for instruction in test_instructions:\n",
        "    print(f\"\\nðŸ“Œ Instruction: {instruction}\")\n",
        "    try:\n",
        "        response = generate_response(model, tokenizer, instruction)\n",
        "        print(f\"   Response: {response[:200]}...\")\n",
        "    except Exception as e:\n",
        "        print(f\"   Error: {e}\")\n",
        "        print(\"   (Model may need to be fine-tuned first)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This lab covered:\n",
        "\n",
        "1. **Dataset Preparation**: Formatting data for instruction tuning\n",
        "2. **Model Loading**: Loading pre-trained LLMs (Llama/DialoGPT)\n",
        "3. **LoRA Configuration**: Setting up parameter-efficient fine-tuning\n",
        "4. **Training Setup**: Configuring training arguments and data collators\n",
        "5. **Fine-tuning**: Training the model on domain-specific data\n",
        "6. **Inference**: Using fine-tuned models for generation\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "- **LoRA is essential**: Reduces memory and training time significantly\n",
        "- **Data quality matters**: Well-formatted prompts lead to better results\n",
        "- **Start small**: Test with small models before scaling up\n",
        "- **Monitor training**: Watch for overfitting and adjust accordingly\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Experiment with different LoRA configurations (r, alpha values)\n",
        "- Try different prompt formats\n",
        "- Fine-tune on your own domain-specific datasets\n",
        "- Explore other PEFT methods (AdaLoRA, QLoRA)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
